{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-26T11:45:51.100892500Z",
     "start_time": "2024-04-26T11:45:45.143696800Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import list_datasets\n",
    "from requests_cache import CachedSession\n",
    "import requests\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Extraction des données des datasets d'HuggingFace</h3>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdc02e0adba4c912"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>1) Extraction des métadonnées</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1a899050a79d39"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fonction pour récupérer les informations (métadonnées) d'un dataset\n",
    "    name : nom du dataset\n",
    "    session : session avec cache\n",
    "\"\"\"\n",
    "def fetch_dataset_info(name, session):\n",
    "    url = f\"https://huggingface.co/api/datasets/{name}\"\n",
    "\n",
    "    while True:\n",
    "        response = session.get(url, params={\"full\": \"True\"})\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        elif response.status_code == 429:\n",
    "            time.sleep(15)\n",
    "        else:\n",
    "            return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T19:09:35.102456300Z",
     "start_time": "2024-04-25T19:09:35.084086900Z"
    }
   },
   "id": "437d16c3864cd4ce"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Fonction pour récupérer les informations (métadonnées) de plusieurs datasets en parallèle\n",
    "    dataset_names : liste des noms des datasets\n",
    "    session : session avec cache\n",
    "    max_workers : nombre de threads\n",
    "\"\"\"\n",
    "def retrieve_dataset_info(dataset_names,session,max_workers=15):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(fetch_dataset_info, name, session) for name in dataset_names]\n",
    "        for future in tqdm(futures, total=len(dataset_names), desc=\"Progress\"):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T12:14:33.174142300Z",
     "start_time": "2024-04-25T12:14:33.145169800Z"
    }
   },
   "id": "f4cbb1961663f162"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antoine\\AppData\\Local\\Temp\\ipykernel_2084\\1704292670.py:2: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  dataset_names = list_datasets()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb total de dataset disponible sur huggingface : 136631\n"
     ]
    }
   ],
   "source": [
    "# Récupération des noms de tous les datasets disponibles sur HuggingFace\n",
    "dataset_names = list_datasets()\n",
    "print(\"nb total de dataset disponible sur huggingface :\", len(dataset_names))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T12:15:11.490036600Z",
     "start_time": "2024-04-25T12:14:34.357531400Z"
    }
   },
   "id": "9e3f4f74ae599257"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 136631/136631 [1:07:37<00:00, 33.67it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb de dataset ayant des métadonnées : 136631\n"
     ]
    }
   ],
   "source": [
    "# Création d'une session avec cache\n",
    "session = CachedSession()\n",
    "# Récupération des informations (métadonnées) des datasets\n",
    "datasets_info = retrieve_dataset_info(dataset_names, session)\n",
    "print(\"nb de dataset ayant des métadonnées :\", len(datasets_info))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-25T12:15:11.492035800Z"
    }
   },
   "id": "f908fd01f032ac06"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Création d'un fichier json avec les données des datasets\n",
    "with open(\"datasets_info.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(datasets_info, f, ensure_ascii=False, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T13:26:45.780615600Z",
     "start_time": "2024-04-25T13:23:30.821735Z"
    }
   },
   "id": "6e0c1633fc840e03"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>2) Nettoyage des métadonnées</h4>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2c0e8af65a29969"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136631/136631 [00:21<00:00, 6298.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_info Nettoyé :\n",
      "{'_id': '621ffdd236468d709f181d58', 'id': 'acronym_identification', 'sha': '15ef643450d589d5883e289ffadeb03563e80a9e', 'lastModified': '2024-01-09T11:39:57.000Z', 'private': False, 'gated': False, 'disabled': False, 'description': '\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Acronym Identification Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains the training, validation, and test data for the Shared Task 1: Acronym Identification of the AAAI-21 Workshop on Scientific Document Understanding.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThe dataset supports an acronym-identification task, where the aim is to predic which tokens in a pre-tokenized sentence correspond to acronyms. The dataset was released for a Shared… See the full description on the dataset page: https://huggingface.co/datasets/acronym_identification.', 'paperswithcode_id': 'acronym-identification', 'downloads': 632, 'likes': 18, 'cardData': {'annotations_creators': ['expert-generated'], 'language_creators': ['found'], 'language': ['en'], 'license': ['mit'], 'multilinguality': ['monolingual'], 'size_categories': ['10K<n<100K'], 'source_datasets': ['original'], 'task_categories': ['token-classification'], 'task_ids': [], 'paperswithcode_id': 'acronym-identification', 'pretty_name': 'Acronym Identification Dataset', 'tags': ['acronym-identification'], 'dataset_info': {'features': [{'name': 'id', 'dtype': 'string'}, {'name': 'tokens', 'sequence': 'string'}, {'name': 'labels', 'sequence': {'class_label': {'names': {'0': 'B-long', '1': 'B-short', '2': 'I-long', '3': 'I-short', '4': 'O'}}}}], 'splits': [{'name': 'train', 'num_bytes': 7792771, 'num_examples': 14006}, {'name': 'validation', 'num_bytes': 952689, 'num_examples': 1717}, {'name': 'test', 'num_bytes': 987712, 'num_examples': 1750}], 'download_size': 2071007, 'dataset_size': 9733172}, 'train-eval-index': [{'config': 'default', 'task': 'token-classification', 'task_id': 'entity_extraction', 'splits': {'eval_split': 'test'}, 'col_mapping': {'tokens': 'tokens', 'labels': 'tags'}}]}, 'createdAt': '2022-03-02T23:29:22.000Z', 'arxiv': ['2010.14678']}\n",
      "Nombre de datasets nettoyés : 133172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#with open(\"datasets_info.json\", \"r\", encoding=\"utf8\") as f:\n",
    "#   datasets_info = json.load(f)\n",
    "\n",
    "# Nettoyage des données\n",
    "datasets_info_clean = []\n",
    "for dataset in tqdm(datasets_info):\n",
    "    arxiv_list = []\n",
    "    if dataset is None:\n",
    "        continue\n",
    "    else:\n",
    "        # On enlève les données inutiles pour réduire la taille du fichier json\n",
    "        if \"cardData\" in dataset and \"configs\" in dataset[\"cardData\"]:\n",
    "            dataset[\"cardData\"].pop(\"configs\")\n",
    "        if \"siblings\" in dataset:\n",
    "            dataset.pop(\"siblings\")\n",
    "        if \"tags\" in dataset:\n",
    "            for tag in dataset[\"tags\"]:\n",
    "                tag = tag.split(':')\n",
    "                if tag[0] == \"arxiv\" and len(tag) == 2:\n",
    "                    arxiv_list.append(tag[1])\n",
    "        dataset.pop(\"tags\")\n",
    "        if arxiv_list:\n",
    "            dataset[\"arxiv\"] = arxiv_list\n",
    "        datasets_info_clean.append(dataset)\n",
    "\n",
    "print(\"dataset_info Nettoyé :\")\n",
    "print(datasets_info_clean[0])\n",
    "print(\"Nombre de datasets nettoyés :\", len(datasets_info_clean))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T13:29:33.612014200Z",
     "start_time": "2024-04-25T13:29:11.884740100Z"
    }
   },
   "id": "8bfa6ac0ca4c6b30"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "with open(\"datasets_info.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(datasets_info_clean, f, ensure_ascii=False, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T13:30:23.895979Z",
     "start_time": "2024-04-25T13:30:14.494985100Z"
    }
   },
   "id": "cce17190ffba17c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>3) Extraction des dataset_cards</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1e62cabe303ce4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fonction pour télécharger le fichier README.md(datasetCard) d'un dataset\n",
    "    repo_id : nom du dataset\n",
    "\"\"\"\n",
    "def download_dataset_card(repo_id):\n",
    "    try:\n",
    "        data_card_path = hf_hub_download(repo_id=repo_id, filename=\"README.md\", repo_type=\"dataset\")\n",
    "        with open(data_card_path, \"r\", encoding=\"utf8\") as f:\n",
    "            dataset_card = f.read()\n",
    "        return dataset_card\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur téléchargement datasetCard {repo_id} :\", str(e))\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T20:04:33.280376200Z",
     "start_time": "2024-04-25T20:04:33.268521700Z"
    }
   },
   "id": "6a8cbcdd3437e264"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fonction pour récupérer les dataset cards\n",
    "    datasets_info : liste contenant les métadonnées des datasets\n",
    "\"\"\"\n",
    "def retrieve_datasets_card(datasets_info):\n",
    "    nb_datasets_card = 0\n",
    "\n",
    "    # Liste pour stocker les futurs résultats\n",
    "    futures = []\n",
    "    # Liste pour stocker les datasetCards\n",
    "    dataset_card_list = []\n",
    "\n",
    "    # Utilisation de ThreadPoolExecutor pour télécharger en parallèle\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        for dataset_info in datasets_info:\n",
    "            if dataset_info and \"cardData\" in dataset_info:\n",
    "                # Soumettre une tâche de téléchargement pour chaque dataset\n",
    "                future = executor.submit(download_dataset_card, dataset_info[\"id\"])\n",
    "                futures.append((dataset_info[\"id\"], future))\n",
    "\n",
    "        # Attendre que toutes les tâches soient terminées\n",
    "        for dataset_id, future in tqdm(futures, total=len(futures), desc=\"Progress\"):\n",
    "            dataset_card = future.result()\n",
    "            if dataset_card:\n",
    "                # Ajouter le dataset_card aux métadonnées du dataset\n",
    "                dataset_card_list.append({\"id\":dataset_id, \"dataset_card\":dataset_card})\n",
    "                nb_datasets_card += 1\n",
    "    \n",
    "    print(f\"Nombre de dataset_card récupérés : {nb_datasets_card} / {len(datasets_info)}\")\n",
    "    \n",
    "    return dataset_card_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T20:04:35.147925700Z",
     "start_time": "2024-04-25T20:04:35.127880500Z"
    }
   },
   "id": "53f962ee256083d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   4%|▍         | 3937/90543 [01:06<23:23, 61.72it/s]  "
     ]
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/4.05k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b9268d02a32461b8acbe7517ea98568"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/5.06k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4dd7db3e7404510be2a13fa331c4525"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/4.90k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9f06e198b8c4c58b988334ae329b260"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   6%|▌         | 5036/90543 [01:23<20:48, 68.48it/s]"
     ]
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/404 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66de54907f3b485ab4cb1cbefdb37f69"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/397 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "daee6e0efd5a4fcf9f402aeca24a31f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   6%|▌         | 5230/90543 [01:27<23:55, 59.42it/s]"
     ]
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/202 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "353de75f87bc4102841ceda32eb40dc9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   6%|▋         | 5683/90543 [01:34<21:12, 66.69it/s]"
     ]
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/422 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d4d0a584f4f42eb8de747e3d4e43c94"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   6%|▋         | 5883/90543 [01:37<22:54, 61.60it/s]"
     ]
    }
   ],
   "source": [
    "with open(\"datasets_info.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    datasets_info = json.load(f)\n",
    "\n",
    "# Désactivation des warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# Récupération des dataset cards\n",
    "datasets_card = retrieve_datasets_card(datasets_info)\n",
    "\n",
    "# Création d'un fichier json avec les métadonnées des datasets + les dataset_card\n",
    "with open(\"datasets_card.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(datasets_card, f, ensure_ascii=False, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-25T20:04:37.691656600Z"
    }
   },
   "id": "482aec5c72a570ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Traitement des datasetCards</h3>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4644bd2b3f810f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>1) Extraction de la description des datasetCards</h4>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66a75d143d9dcb2c"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_description_from_card(model_card):\n",
    "    description_keywords =\\\n",
    "    [\"Description\", \"description\", \"Summary\", \"summary\", \"Detail\", \"detail\", \"Dataset\", \"dataset\"]\n",
    "    \n",
    "    description = \"\"\n",
    "    lines = model_card.split(\"\\n\")\n",
    "    \n",
    "    for i in range(len(lines)):\n",
    "        if lines[i].startswith(\"#\"):\n",
    "            for description_keyword in description_keywords:\n",
    "                if description_keyword in lines[i]:\n",
    "                    # On récupère la description du dataset\n",
    "                    i+=1\n",
    "                    while i < len(lines) and not lines[i].startswith(\"#\"):\n",
    "                        description = description + lines[i]\n",
    "                        i+=1\n",
    "                    return description\n",
    "    return None "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T19:10:29.704782900Z",
     "start_time": "2024-04-25T19:10:29.691596700Z"
    }
   },
   "id": "686b1be317cc0a9f"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90540/90540 [00:01<00:00, 55167.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40298\n",
      "90540\n",
      "Pourcentage de dataset cards avec description: 44.51%\n",
      "[{'description': '<div class=\"course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400\"><p><b>Deprecated:</b> Dataset \"common_voice\" is deprecated and will soon be deleted. Use datasets under <a href=\"https://huggingface.co/mozilla-foundation\">mozilla-foundation</a> organisation instead. For example, you can load <a href=\"https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0\">Common Voice 13</a> dataset via <code>load_dataset(\"mozilla-foundation/common_voice_13_0\", \"en\")</code></p></div>', 'id': 'common_voice'}, {'description': ' ', 'id': 'freebase_qa'}, {'description': '**BAAD16** is an **Authorship Attribution dataset for Bengali Literature**. It was collected and analyzed by the authors of [this paper](https://arxiv.org/abs/2001.05316). It was created by scraping text from an online Bangla e-library using custom web crawler and contains literary works of various famous Bangla writers. It contains novels, stories, series, and other works of 16 authors. Each sample document is created with 750 words. The dataset is imbalanced and resembles real-world scenarios more closely, where not all the authors will have a large number of sample texts. The following table gives more details about the dataset.| Author Name | Number of Samples | Word Count | Unique Word| --- | --- | --- | --- || zahir rayhan        | 185              | 138k | 20k|nazrul               | 223              | 167k | 33k|manik bandhopaddhay  | 469              | 351k | 44k|nihar ronjon gupta   | 476              | 357k | 43k|bongkim              | 562              | 421k | 62k|tarashonkor          | 775              | 581k | 84k|shottojit roy        | 849              | 636k | 67k|shordindu            | 888              | 666k | 84k|toslima nasrin       | 931              | 698k | 76k|shirshendu           | 1048             | 786k | 69k|zafar iqbal          | 1100             | 825k | 53k|robindronath         | 1259             | 944k | 89k|shorotchandra        | 1312             | 984k | 78k|shomresh             | 1408             | 1056k|69k|shunil gongopaddhay  | 1963             | 1472k|109k|humayun ahmed        | 4518             | 3388k |161k**Total**| 17,966|13,474,500 | 590,660**Average**|1,122.875|842,156.25| 71,822.25    ', 'id': 'Aisha/BAAD16'}, {'description': '**BAAD6** is an **Authorship Attribution dataset for Bengali Literature**. It was collected and analyzed by Hemayet et al [[1]](https://ieeexplore.ieee.org/document/8631977). The data was obtained from different online posts and blogs. This dataset is balanced among the 6 Authors with 350 sample texts per author. This is a relatively small dataset but is noisy given the sources it was collected from and its cleaning procedure. Nonetheless, it may help evaluate authorship attribution systems as it resembles texts often available on the Internet. Details about the dataset are given in the table below.| Author | Samples | Word count | Unique word || ------ | ------ | ------ | ------ ||fe|350|357k|53k|| ij | 350 | 391k | 72k| mk | 350 | 377k | 47k| rn | 350 | 231k | 50k| hm | 350 | 555k | 72k| rg | 350 | 391k | 58k**Total** | 2,100 | 2,304,338 | 230,075**Average** | 350 | 384,056.33 | 59,006.67', 'id': 'Aisha/BAAD6'}, {'description': '👉 https://observablehq.com/@frasergreenlee/python-lines-dataset#chart', 'id': 'Fraser/python-state-changes'}, {'description': 'The dataset quora_swe is a subset of the automatically translated (MNT) Swedish Semantic Textual Similarity dataset: quora-deduplicates .', 'id': 'Gabriel/quora_swe'}, {'description': '![HUPD-Diagram](https://huggingface.co/datasets/HUPD/hupd/resolve/main/HUPD-Logo.png)', 'id': 'HUPD/hupd'}, {'description': 'Dataset Description:- **Homepage:** https://metatext.io/datasets/wikiner- **Repository:** - **Paper:** https://www.sciencedirect.com/science/article/pii/S0004370212000276?via%3Dihub- **Leaderboard:**- **Point of Contact:**', 'id': 'Jean-Baptiste/wikiner_fr'}, {'description': 'We provide sentential paraphrase detection train, test datasets as well as BERT-based models for the Armenian language.', 'id': 'Karavet/ARPA-Armenian-Paraphrase-Corpus'}, {'description': 'We release a dataset of over 12000 news articles from [iLur.am](http://www.ilur.am/), categorized into 7 classes: sport, politics, weather, economy, accidents, art, society. The articles are split into train (2242k tokens) and test sets (425k tokens).For more details, refer to the [paper](https://arxiv.org/ftp/arxiv/papers/1906/1906.03134.pdf).', 'id': 'Karavet/ILUR-news-text-classification-corpus'}, {'description': 'pioNER corpus provides gold-standard and automatically generated named-entity datasets for the Armenian language.Alongside the datasets, we release 50-, 100-, 200-, and 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.', 'id': 'Karavet/pioNER-Armenian-Named-Entity'}, {'description': 'SQAC is an extractive QA dataset for the Spanish language.- **Paper:** [MarIA: Spanish Language Models](https://upcommons.upc.edu/bitstream/handle/2117/367156/6405-5863-1-PB%20%281%29.pdf?sequence=1)- **Point of Contact:** carlos.rodriguez1@bsc.es- **Leaderboard:** [EvalEs] (https://plantl-gob-es.github.io/spanish-benchmark/)', 'id': 'PlanTL-GOB-ES/SQAC'}, {'description': 'Manually classified collection of Spanish oncological clinical case reports.- **Homepage:** [zenodo](https://zenodo.org/record/3978041)- **Paper:** [Named Entity Recognition, Concept Normalization and Clinical Coding: Overview of the Cantemist Track for Cancer Text Mining in Spanish, Corpus, Guidelines, Methods and Results](https://www.researchgate.net/profile/Antonio-Miranda-Escalada-2/publication/352786464_Named_Entity_Recognition_Concept_Normalization_and_Clinical_Coding_Overview_of_the_Cantemist_Track_for_Cancer_Text_Mining_in_Spanish_Corpus_Guidelines_Methods_and_Results/links/60d98a3b458515d6fbe382d8/Named-Entity-Recognition-Concept-Normalization-and-Clinical-Coding-Overview-of-the-Cantemist-Track-for-Cancer-Text-Mining-in-Spanish-Corpus-Guidelines-Methods-and-Results.pdf)- **Point of Contact:** encargo-pln-life@bsc.es', 'id': 'PlanTL-GOB-ES/cantemist-ner'}, {'description': 'Manually classified collection of Spanish clinical case studies.- **Homepage:** [zenodo](https://zenodo.org/record/4270158)- **Paper:** [PharmaCoNER: Pharmacological Substances, Compounds and proteins Named Entity Recognition track](https://aclanthology.org/D19-5701/)- **Point of Contact:** encargo-pln-life@bsc.es', 'id': 'PlanTL-GOB-ES/pharmaconer'}, {'description': 'persian_news_dataset is a collection of 400k blog posts. these posts have been gathered from more than 10 websites. This dataset can be used in different NLP tasks like language modeling and text generation tasks.This effort is part of a bigger perspective to have several datasets in Persian language for different tasks that have two important factors: `free` and `easy-to-use`. Here is a quick HOW-TO for using this dataset in datasets library:[Demo-datasets](https://saied71.github.io/RohanAiLab/2021/09/03/Demo-datasets.html)', 'id': 'RohanAiLab/persian_blog'}, {'description': 'persian_daily_news  is a collection of 2 million of unique news articles with the headline for each article. dataset can be used in abstractive summarization and paraphrasing tasks.This effort is part of a bigger perspective to have several datasets in Persian language(and other low resources languages) for different tasks that have two important factors: `free` and `easy-to-use`. Here is a quick HOW-TO for using this dataset in datasets library:[Demo-datasets](https://saied71.github.io/RohanAiLab/2021/09/03/Demo-datasets.html)', 'id': 'RohanAiLab/persian_daily_news'}, {'description': 'It includes list of Arabic names with meaning and origin of most names', 'id': 'TRoboto/names'}, {'description': 'Contains aggregated dataset of Russian texts from 6 datasets.', 'id': 'MonoHime/ru_sentiment_dataset'}, {'description': 'Polish CDSCorpus consists of 10K Polish sentence pairs which are human-annotated for semantic relatedness (**CDSC-R**) and entailment (**CDSC-E**). The dataset may be used to evaluate compositional distributional semantics models of Polish. The dataset was presented at ACL 2017. Although the SICK corpus inspires the main design of the dataset, it differs in detail. As in SICK, the sentences come from image captions, but the set of chosen images is much more diverse as they come from 46 thematic groups.', 'id': 'allegro/klej-cdsc-e'}, {'description': 'The Czy wiesz? (eng. Did you know?) the dataset consists of almost 5k question-answer pairs obtained from Czy wiesz... section of Polish Wikipedia. Each question is written by a Wikipedia collaborator and is answered with a link to a relevant Wikipedia article. In huggingface version of this dataset, they chose the negatives which have the largest token overlap with a question.', 'id': 'allegro/klej-dyk'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Parcourir le fichier json des datasets pour récupérer les dataset cards\n",
    "with open(\"datasets_card.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    datasets = json.load(f)\n",
    "\n",
    "descriptions = []\n",
    "nb_datasets = len(datasets)\n",
    "nb_descriptions_found = 0\n",
    "\n",
    "# Récupération des descriptions des dataset cards\n",
    "for dataset in tqdm(datasets):\n",
    "    #pq datataset peut être None\n",
    "    if dataset is None:\n",
    "        continue\n",
    "    if \"dataset_card\" in dataset:\n",
    "        dataset_card = dataset[\"dataset_card\"]\n",
    "        description = get_description_from_card(dataset_card)\n",
    "        if description:\n",
    "            descriptions.append({\"description\":description, \"id\":dataset[\"id\"]})\n",
    "            nb_descriptions_found += 1\n",
    "\n",
    "print(nb_descriptions_found)\n",
    "print(nb_datasets)\n",
    "print(f\"Pourcentage de dataset cards avec description: {(nb_descriptions_found/nb_datasets)*100:.2f}%\")\n",
    "print(descriptions[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T19:10:42.806383700Z",
     "start_time": "2024-04-25T19:10:34.462460300Z"
    }
   },
   "id": "fdaf82eedb30f117"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Récupération d'information académiques</h3>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c7e5fc1801e80c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>1) Récupération des papiers arxiv</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d926b32928ff818"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import xmltodict\n",
    "\"\"\"\n",
    "Fonction pour télécharger le Papier de recherche correspondant à un arxiv cité dans les méta-données du dataset\n",
    "    arxiv : numéro de l'arxiv\n",
    "\"\"\"\n",
    "def download_arxiv_paper(arxiv):\n",
    "    try:\n",
    "        url = f\"http://export.arxiv.org/api/query?max_results=1&search_query=all:{arxiv}\"\n",
    "        data = urllib.request.urlopen(url)\n",
    "        # Convertir les données XML en dictionnaire Python\n",
    "        xml_data = data.read().decode('utf-8')\n",
    "        dict_data = xmltodict.parse(xml_data)\n",
    "        return dict_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur téléchargement du papier arxiv:{arxiv} :\", str(e))\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T22:32:21.828311Z",
     "start_time": "2024-04-25T22:32:21.781084600Z"
    }
   },
   "id": "13215323207e6205"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fonction pour récupérer les papiers de recherche arxiv\n",
    "    datasets_info : liste contenant les métadonnées des datasets (notamment les numéros arxiv)\n",
    "\"\"\"\n",
    "def retrieve_arxiv_paper(datasets_info):\n",
    "    nb_arxiv_paper = 0\n",
    "    nb_arxiv_number = 0\n",
    "\n",
    "    # Liste pour stocker les futurs résultats\n",
    "    futures = []\n",
    "    # Liste pour stocker les datasetCards\n",
    "    arxiv_paper_list = []\n",
    "\n",
    "    # Utilisation de ThreadPoolExecutor pour télécharger en parallèle\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        for dataset_info in datasets_info:\n",
    "            if \"arxiv\" in dataset_info:\n",
    "                for arxiv in dataset_info[\"arxiv\"]:\n",
    "                    nb_arxiv_number += 1\n",
    "                    # Soumettre une tâche de téléchargement pour chaque arxiv\n",
    "                    future = executor.submit(download_arxiv_paper, arxiv)\n",
    "                    futures.append((dataset_info[\"id\"], arxiv, future))\n",
    "\n",
    "        # Attendre que toutes les tâches soient terminées\n",
    "        for dataset_id, arxiv, future in tqdm(futures, total=len(futures), desc=\"Progress\"):\n",
    "            paper_arxiv = future.result()\n",
    "            if paper_arxiv:\n",
    "                arxiv_paper_list.append({\"id\":dataset_id, \"arxiv\":arxiv, \"paper_arxiv\":paper_arxiv})\n",
    "                nb_arxiv_paper += 1\n",
    "    \n",
    "    print(f\"nb de paper_arxiv récupérés / nb de numéro arxiv récupérés : {nb_arxiv_paper} / {nb_arxiv_number}\")\n",
    "    \n",
    "    return arxiv_paper_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T22:32:38.326559600Z",
     "start_time": "2024-04-25T22:32:38.293385700Z"
    }
   },
   "id": "5c2cbd7d6e4e69c7"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Lecture du fichier json\n",
    "with open(\"datasets_info.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    datasets_info = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T22:32:46.723791100Z",
     "start_time": "2024-04-25T22:32:40.993311400Z"
    }
   },
   "id": "e640acc55d69af59"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de datasets ayant des numéros arxiv : 3581/133172\n"
     ]
    }
   ],
   "source": [
    "# Compte le nombre de datasets ayant des numéros arxiv\n",
    "nb_datasets_with_arxiv = 0\n",
    "for dataset in datasets_info:\n",
    "    if \"arxiv\" in dataset:\n",
    "        nb_datasets_with_arxiv += 1\n",
    "print(f\"Nombre de datasets ayant des numéros arxiv : {nb_datasets_with_arxiv}/{len(datasets_info)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T22:32:53.166892700Z",
     "start_time": "2024-04-25T22:32:53.074712800Z"
    }
   },
   "id": "69b3a8873dd11982"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retrieve_arxiv_paper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Récupération des papiers de recherche arxiv\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m arxiv_paper_list \u001B[38;5;241m=\u001B[39m \u001B[43mretrieve_arxiv_paper\u001B[49m(datasets_info)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Création d'un fichier json contenant les papiers de recherche arxiv\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marxiv_paper.json\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "\u001B[1;31mNameError\u001B[0m: name 'retrieve_arxiv_paper' is not defined"
     ]
    }
   ],
   "source": [
    "# Récupération des papiers de recherche arxiv\n",
    "arxiv_paper_list = retrieve_arxiv_paper(datasets_info)\n",
    "\n",
    "# Création d'un fichier json contenant les papiers de recherche arxiv\n",
    "with open(\"arxiv_paper.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(arxiv_paper_list, f, ensure_ascii=False, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T22:48:50.214125800Z",
     "start_time": "2024-04-25T22:48:49.976747100Z"
    }
   },
   "id": "b4c4ee944b62a5c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>2) Nettoyage des papiers arxiv</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cf1870de93ca40b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "with open(\"arxiv_paper.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    arxiv_paper_list = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T22:49:03.580317Z",
     "start_time": "2024-04-25T22:49:03.442327200Z"
    }
   },
   "id": "d764b87cf97cdab6"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de papiers arxiv nettoyés : 2020\n"
     ]
    }
   ],
   "source": [
    "new_arxiv_paper_list = []\n",
    "for arxiv_paper in arxiv_paper_list:\n",
    "    if arxiv_paper and arxiv_paper[\"paper_arxiv\"] and \"feed\" in arxiv_paper[\"paper_arxiv\"] and \"entry\" in arxiv_paper[\"paper_arxiv\"][\"feed\"]:\n",
    "        new_arxiv_paper_list.append({\"id\": arxiv_paper[\"id\"], \"arxiv\": arxiv_paper[\"arxiv\"], \"paper_arxiv\": arxiv_paper[\"paper_arxiv\"][\"feed\"][\"entry\"]})\n",
    "\n",
    "print(\"Nombre de papiers arxiv nettoyés :\", len(new_arxiv_paper_list))\n",
    "with open(\"arxiv_paper.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(new_arxiv_paper_list, f, ensure_ascii=False, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T22:52:27.648872500Z",
     "start_time": "2024-04-25T22:52:27.354670200Z"
    }
   },
   "id": "9296c785b2726cab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>3) Récupération des informations sur les citations (Serpapi)</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7975a6a55de61259"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'search_metadata': {'id': '662b9ad55fc49355de6308ca', 'status': 'Success', 'json_endpoint': 'https://serpapi.com/searches/dba6a72319338321/662b9ad55fc49355de6308ca.json', 'created_at': '2024-04-26 12:15:17 UTC', 'processed_at': '2024-04-26 12:15:17 UTC', 'google_scholar_url': 'https://scholar.google.com/scholar?q=arXiv%3A1909.11942&hl=en&num=1', 'raw_html_file': 'https://serpapi.com/searches/dba6a72319338321/662b9ad55fc49355de6308ca.html', 'total_time_taken': 0.98}, 'search_parameters': {'engine': 'google_scholar', 'q': 'arXiv:1909.11942', 'hl': 'en', 'num': '1'}, 'search_information': {'organic_results_state': 'Results for exact spelling', 'query_displayed': 'arXiv:1909.11942'}, 'profiles': {'link': 'https://scholar.google.com/scholar?lookup=0&q=arXiv:1909.11942&hl=en&num=1&as_sdt=0,11', 'serpapi_link': 'https://serpapi.com/search.json?engine=google_scholar_profiles&hl=en&mauthors=arXiv%3A1909.11942'}, 'organic_results': [{'position': 0, 'title': 'Albert: A lite bert for self-supervised learning of language representations', 'result_id': 'wzWRMzbJr1sJ', 'link': 'https://arxiv.org/abs/1909.11942', 'snippet': 'Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the …', 'publication_info': {'summary': 'Z Lan, M Chen, S Goodman, K Gimpel… - arXiv preprint arXiv …, 2019 - arxiv.org', 'authors': [{'name': 'Z Lan', 'link': 'https://scholar.google.com/citations?user=tlDABkgAAAAJ&hl=en&num=1&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=tlDABkgAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'tlDABkgAAAAJ'}, {'name': 'M Chen', 'link': 'https://scholar.google.com/citations?user=aRncxakAAAAJ&hl=en&num=1&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=aRncxakAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'aRncxakAAAAJ'}, {'name': 'S Goodman', 'link': 'https://scholar.google.com/citations?user=xgZ6V-sAAAAJ&hl=en&num=1&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=xgZ6V-sAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'xgZ6V-sAAAAJ'}, {'name': 'K Gimpel', 'link': 'https://scholar.google.com/citations?user=kDHs7DYAAAAJ&hl=en&num=1&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=kDHs7DYAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'kDHs7DYAAAAJ'}]}, 'resources': [{'title': 'arxiv.org', 'file_format': 'PDF', 'link': 'https://arxiv.org/pdf/1909.11942.pdf%3E,'}], 'inline_links': {'serpapi_cite_link': 'https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=wzWRMzbJr1sJ', 'cited_by': {'total': 6757, 'link': 'https://scholar.google.com/scholar?cites=6606720413006378435&as_sdt=80005&sciodt=0,11&hl=en&num=1', 'cites_id': '6606720413006378435', 'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=80005&cites=6606720413006378435&engine=google_scholar&hl=en&num=1'}, 'related_pages_link': 'https://scholar.google.com/scholar?q=related:wzWRMzbJr1sJ:scholar.google.com/&scioq=arXiv:1909.11942&hl=en&num=1&as_sdt=0,11', 'serpapi_related_pages_link': 'https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=1&q=related%3AwzWRMzbJr1sJ%3Ascholar.google.com%2F', 'versions': {'total': 10, 'link': 'https://scholar.google.com/scholar?cluster=6606720413006378435&hl=en&num=1&as_sdt=0,11', 'cluster_id': '6606720413006378435', 'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=0%2C11&cluster=6606720413006378435&engine=google_scholar&hl=en&num=1'}, 'cached_page_link': 'https://scholar.googleusercontent.com/scholar?q=cache:wzWRMzbJr1sJ:scholar.google.com/+arXiv:1909.11942&hl=en&num=1&as_sdt=0,11'}}]}\n"
     ]
    }
   ],
   "source": [
    "#refered by:\n",
    "# import serpapi\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "params = {\n",
    "    \"engine\": \"google_scholar\",\n",
    "    \"num\": \"1\",\n",
    "    \"q\": \"arXiv:1909.11942\",\n",
    "    \"hl\": \"en\",\n",
    "    \"api_key\": \"5c031d347fae722e2e6576c726ff739ccf6d55165001bb936c95cfa7e15a5994\"\n",
    "}\n",
    "\n",
    "search = GoogleSearch(params)\n",
    "results = search.get_dict()\n",
    "if results:\n",
    "    if \"inline_links\" in results and \"cited_by\" in results[\"inline_links\"]:\n",
    "        print(results[\"inline_links\"][\"cited_by\"])\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T12:18:54.124946500Z",
     "start_time": "2024-04-26T12:18:53.320501300Z"
    }
   },
   "id": "93fabba03d9c01b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>4) Récupération des informations sur les citations (Scholarly)</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8739e0a3f294c311"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Albert: A lite bert for self-supervised learning of language representations', 'author': ['Z Lan', 'M Chen', 'S Goodman', 'K Gimpel'], 'pub_year': '2019', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/1909.11942', 'author_id': ['tlDABkgAAAAJ', 'aRncxakAAAAJ', 'xgZ6V-sAAAAJ', 'kDHs7DYAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:wzWRMzbJr1sJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DarXiv:1909.11942%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wzWRMzbJr1sJ&ei=pqgrZvjdE5SCy9YP29Cc0AY&json=', 'num_citations': 6757, 'citedby_url': '/scholar?cites=6606720413006378435&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:wzWRMzbJr1sJ:scholar.google.com/&scioq=arXiv:1909.11942&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/1909.11942.pdf%3E,'}\n"
     ]
    }
   ],
   "source": [
    "from scholarly import scholarly\n",
    "\n",
    "# Définir le terme de recherche\n",
    "search_query = 'arXiv:1909.11942'\n",
    "# Effectuer la recherche\n",
    "search_results = scholarly.search_pubs(search_query)\n",
    "\n",
    "# Afficher les informations sur les citations pour chaque résultat de la recherche\n",
    "for i, result in enumerate(search_results):\n",
    "    print(result)\n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"Résultat {i+1}:\")\n",
    "    print(\"Année de publication:\", result[\"bib\"]['pub_year'])\n",
    "    print(\"url citation:\", \"https://scholar.google.com\"+result[\"citedby_url\"])\n",
    "    \n",
    "    if \"num_citations\" in result:\n",
    "        print(\"Nombre de citations:\", result[\"num_citations\"])\n",
    "    else:\n",
    "        print(\"Aucune information sur les citations disponible.\")\n",
    "    \n",
    "    results = scholarly.search_citedby(\"6606720413006378435\")\n",
    "    for i, res in enumerate(results):\n",
    "        print(f\"Résultat {i+1}:\")\n",
    "        print(\"Titre:\", res[\"bib\"][\"title\"])\n",
    "        print(\"Année de publication:\", res[\"bib\"]['pub_year'])\n",
    "        \n",
    "        if \"num_citations\" in res:\n",
    "            print(\"Nombre de citations:\", res[\"num_citations\"])\n",
    "        else:\n",
    "            print(\"Aucune information sur les citations disponible.\")\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:14:15.609890600Z",
     "start_time": "2024-04-26T13:14:13.355423800Z"
    }
   },
   "id": "ac9801ece73ee1ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>5) Récupération des papiers de recherche citant le dataset (Serpapi)</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8321d2c329397875"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6606720413006378435\n",
      "Résultat 1:\n",
      "Titre: Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\n",
      "Année de publication: 2023\n",
      "Nombre de citations: 3114\n",
      "Résultat 2:\n",
      "Titre: Deep learning--based text classification: a comprehensive review\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 1475\n",
      "Résultat 3:\n",
      "Titre: Lamda: Language models for dialog applications\n",
      "Année de publication: 2022\n",
      "Nombre de citations: 1096\n",
      "Résultat 4:\n",
      "Titre: Flashattention: Fast and memory-efficient exact attention with io-awareness\n",
      "Année de publication: 2022\n",
      "Nombre de citations: 717\n",
      "Résultat 5:\n",
      "Titre: Vivit: A video vision transformer\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 1817\n",
      "Résultat 6:\n",
      "Titre: On the dangers of stochastic parrots: Can language models be too big?🦜\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 3557\n",
      "Résultat 7:\n",
      "Titre: Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\n",
      "Année de publication: 2022\n",
      "Nombre de citations: 734\n",
      "Résultat 8:\n",
      "Titre: BERTopic: Neural topic modeling with a class-based TF-IDF procedure\n",
      "Année de publication: 2022\n",
      "Nombre de citations: 1094\n",
      "Résultat 9:\n",
      "Titre: A survey on vision transformer\n",
      "Année de publication: 2022\n",
      "Nombre de citations: 1402\n",
      "Résultat 10:\n",
      "Titre: Language models are few-shot learners\n",
      "Année de publication: 2020\n",
      "Nombre de citations: 24464\n",
      "Résultat 11:\n",
      "Titre: Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension\n",
      "Année de publication: 2019\n",
      "Nombre de citations: 9039\n",
      "Résultat 12:\n",
      "Titre: Deberta: Decoding-enhanced bert with disentangled attention\n",
      "Année de publication: 2020\n",
      "Nombre de citations: 1941\n",
      "Résultat 13:\n",
      "Titre: Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 567\n",
      "Résultat 14:\n",
      "Titre: Xlnet: Generalized autoregressive pretraining for language understanding\n",
      "Année de publication: 2019\n",
      "Nombre de citations: 9074\n",
      "Résultat 15:\n",
      "Titre: Exploring the limits of transfer learning with a unified text-to-text transformer\n",
      "Année de publication: 2020\n",
      "Nombre de citations: 14778\n",
      "Résultat 16:\n",
      "Titre: Self-supervised learning: Generative or contrastive\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 1459\n",
      "Résultat 17:\n",
      "Titre: Efficient transformers: A survey\n",
      "Année de publication: 2022\n",
      "Nombre de citations: 1054\n",
      "Résultat 18:\n",
      "Titre: Pre-trained models for natural language processing: A survey\n",
      "Année de publication: 2020\n",
      "Nombre de citations: 1556\n",
      "Résultat 19:\n",
      "Titre: Roformer: Enhanced transformer with rotary position embedding\n",
      "Année de publication: 2024\n",
      "Nombre de citations: 727\n",
      "Résultat 20:\n",
      "Titre: Electra: Pre-training text encoders as discriminators rather than generators\n",
      "Année de publication: 2020\n",
      "Nombre de citations: 3645\n",
      "Résultat 21:\n",
      "Titre: Maxvit: Multi-axis vision transformer\n",
      "Année de publication: 2022\n",
      "Nombre de citations: 390\n",
      "Résultat 22:\n",
      "Titre: Perceiver: General perception with iterative attention\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 741\n",
      "Résultat 23:\n",
      "Titre: Recent advances in natural language processing via large pre-trained language models: A survey\n",
      "Année de publication: 2023\n",
      "Nombre de citations: 438\n",
      "Résultat 24:\n",
      "Titre: Transformers are rnns: Fast autoregressive transformers with linear attention\n",
      "Année de publication: 2020\n",
      "Nombre de citations: 1139\n",
      "Résultat 25:\n",
      "Titre: Glm: General language model pretraining with autoregressive blank infilling\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 686\n",
      "Résultat 26:\n",
      "Titre: Prottrans: Toward understanding the language of life through self-supervised learning\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 1083\n",
      "Résultat 27:\n",
      "Titre: Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 691\n",
      "Résultat 28:\n",
      "Titre: A large language model for electronic health records\n",
      "Année de publication: 2022\n",
      "Nombre de citations: 226\n",
      "Résultat 29:\n",
      "Titre: A comprehensive survey on pretrained foundation models: A history from bert to chatgpt\n",
      "Année de publication: 2023\n",
      "Nombre de citations: 313\n",
      "Résultat 30:\n",
      "Titre: Self-supervised graph learning for recommendation\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 766\n",
      "Résultat 31:\n",
      "Titre: Less is more: Clipbert for video-and-language learning via sparse sampling\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 582\n",
      "Résultat 32:\n",
      "Titre: It's not just size that matters: Small language models are also few-shot learners\n",
      "Année de publication: 2020\n",
      "Nombre de citations: 769\n",
      "Résultat 33:\n",
      "Titre: Uniter: Universal image-text representation learning\n",
      "Année de publication: 2020\n",
      "Nombre de citations: 1884\n",
      "Résultat 34:\n",
      "Titre: Pre-trained models: Past, present and future\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 586\n",
      "Résultat 35:\n",
      "Titre: Underspecification presents challenges for credibility in modern machine learning\n",
      "Année de publication: 2022\n",
      "Nombre de citations: 666\n",
      "Résultat 36:\n",
      "Titre: A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt\n",
      "Année de publication: 2023\n",
      "Nombre de citations: 362\n",
      "Résultat 37:\n",
      "Titre: A primer in BERTology: What we know about how BERT works\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 1452\n",
      "Résultat 38:\n",
      "Titre: Llm-pruner: On the structural pruning of large language models\n",
      "Année de publication: 2023\n",
      "Nombre de citations: 120\n",
      "Résultat 39:\n",
      "Titre: Tinybert: Distilling bert for natural language understanding\n",
      "Année de publication: 2019\n",
      "Nombre de citations: 1603\n",
      "Résultat 40:\n",
      "Titre: An empirical study of training end-to-end vision-and-language transformers\n",
      "Année de publication: 2022\n",
      "Nombre de citations: 293\n",
      "Résultat 41:\n",
      "Titre: Vision-language pre-training: Basics, recent advances, and future trends\n",
      "Année de publication: 2022\n",
      "Nombre de citations: 125\n",
      "Résultat 42:\n",
      "Titre: A brief overview of ChatGPT: The history, status quo and potential future development\n",
      "Année de publication: 2023\n",
      "Nombre de citations: 263\n",
      "Résultat 43:\n",
      "Titre: LUKE: Deep contextualized entity representations with entity-aware self-attention\n",
      "Année de publication: 2020\n",
      "Nombre de citations: 664\n",
      "Résultat 44:\n",
      "Titre: Spanish pre-trained bert model and evaluation data\n",
      "Année de publication: 2023\n",
      "Nombre de citations: 791\n",
      "Résultat 45:\n",
      "Titre: LEGAL-BERT: The muppets straight out of law school\n",
      "Année de publication: 2020\n",
      "Nombre de citations: 673\n",
      "Résultat 46:\n",
      "Titre: Zeroquant: Efficient and affordable post-training quantization for large-scale transformers\n",
      "Année de publication: 2022\n",
      "Nombre de citations: 180\n",
      "Résultat 47:\n",
      "Titre: ABCDM: An attention-based bidirectional CNN-RNN deep model for sentiment analysis\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 610\n",
      "Résultat 48:\n",
      "Titre: Unifying vision-and-language tasks via text generation\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 440\n",
      "Résultat 49:\n",
      "Titre: Megatron-lm: Training multi-billion parameter language models using model parallelism\n",
      "Année de publication: 2019\n",
      "Nombre de citations: 1288\n",
      "Résultat 50:\n",
      "Titre: Revisiting pre-trained models for Chinese natural language processing\n",
      "Année de publication: 2020\n",
      "Nombre de citations: 650\n",
      "Résultat 51:\n",
      "Titre: QA-GNN: Reasoning with language models and knowledge graphs for question answering\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 440\n",
      "Résultat 52:\n",
      "Titre: Pre-training with whole word masking for chinese bert\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 1210\n",
      "Résultat 53:\n",
      "Titre: ProteinBERT: a universal deep-learning model of protein sequence and function\n",
      "Année de publication: 2022\n",
      "Nombre de citations: 336\n",
      "Résultat 54:\n",
      "Titre: An introduction to deep learning in natural language processing: Models, techniques, and tools\n",
      "Année de publication: 2022\n",
      "Nombre de citations: 324\n",
      "Résultat 55:\n",
      "Titre: Agentformer: Agent-aware transformers for socio-temporal multi-agent forecasting\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 355\n",
      "Résultat 56:\n",
      "Titre: Siren's song in the AI ocean: a survey on hallucination in large language models\n",
      "Année de publication: 2023\n",
      "Nombre de citations: 277\n",
      "Résultat 57:\n",
      "Titre: Graph neural networks: foundation, frontiers and applications\n",
      "Année de publication: 2022\n",
      "Nombre de citations: 273\n",
      "Résultat 58:\n",
      "Titre: Vitae: Vision transformer advanced by exploring intrinsic inductive bias\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 290\n",
      "Résultat 59:\n",
      "Titre: True few-shot learning with language models\n",
      "Année de publication: 2021\n",
      "Nombre de citations: 312\n",
      "Résultat 60:\n",
      "Titre: Merlot reserve: Neural script knowledge through vision and language and sound\n",
      "Année de publication: 2022\n",
      "Nombre de citations: 195\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(cites_id_match\u001B[38;5;241m.\u001B[39mgroup(\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m      7\u001B[0m results \u001B[38;5;241m=\u001B[39m scholarly\u001B[38;5;241m.\u001B[39msearch_citedby(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m6606720413006378435\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mres\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mresults\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mRésultat \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m:\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTitre:\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mres\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbib\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtitle\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scholarly\\publication_parser.py:90\u001B[0m, in \u001B[0;36m_SearchScholarIterator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     87\u001B[0m     url \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_soup\u001B[38;5;241m.\u001B[39mfind(\n\u001B[0;32m     88\u001B[0m         class_\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgs_ico gs_ico_nav_next\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mparent[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhref\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     89\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_url \u001B[38;5;241m=\u001B[39m url\n\u001B[1;32m---> 90\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_url\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     91\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__next__\u001B[39m()\n\u001B[0;32m     92\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scholarly\\publication_parser.py:59\u001B[0m, in \u001B[0;36m_SearchScholarIterator._load_url\u001B[1;34m(self, url)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load_url\u001B[39m(\u001B[38;5;28mself\u001B[39m, url: \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;66;03m# this is temporary until setup json file\u001B[39;00m\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_soup \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nav\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_soup\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pos \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rows \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_soup\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiv\u001B[39m\u001B[38;5;124m'\u001B[39m, class_\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgs_r gs_or gs_scl\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_soup\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiv\u001B[39m\u001B[38;5;124m'\u001B[39m, class_\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgsc_mpat_ttl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scholarly\\_navigator.py:239\u001B[0m, in \u001B[0;36mNavigator._get_soup\u001B[1;34m(self, url)\u001B[0m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_soup\u001B[39m(\u001B[38;5;28mself\u001B[39m, url: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BeautifulSoup:\n\u001B[0;32m    238\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the BeautifulSoup for a page on scholar.google.com\"\"\"\u001B[39;00m\n\u001B[1;32m--> 239\u001B[0m     html \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_page\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhttps://scholar.google.com\u001B[39;49m\u001B[38;5;132;43;01m{0}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    240\u001B[0m     html \u001B[38;5;241m=\u001B[39m html\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\xa0\u001B[39;00m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    241\u001B[0m     res \u001B[38;5;241m=\u001B[39m BeautifulSoup(html, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhtml.parser\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scholarly\\_navigator.py:132\u001B[0m, in \u001B[0;36mNavigator._get_page\u001B[1;34m(self, pagerequest, premium)\u001B[0m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m has_captcha:\n\u001B[0;32m    131\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGot a captcha request.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 132\u001B[0m     session \u001B[38;5;241m=\u001B[39m \u001B[43mpm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_captcha2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpagerequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# Retry request within same session\u001B[39;00m\n\u001B[0;32m    134\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m403\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scholarly\\_proxy_generator.py:404\u001B[0m, in \u001B[0;36mProxyGenerator._handle_captcha2\u001B[1;34m(self, url)\u001B[0m\n\u001B[0;32m    403\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_handle_captcha2\u001B[39m(\u001B[38;5;28mself\u001B[39m, url):\n\u001B[1;32m--> 404\u001B[0m     cur_host \u001B[38;5;241m=\u001B[39m urlparse(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_webdriver\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcurrent_url)\u001B[38;5;241m.\u001B[39mhostname\n\u001B[0;32m    405\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m cookie \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mcookies:\n\u001B[0;32m    406\u001B[0m         \u001B[38;5;66;03m# Only set cookies matching the current domain, cf. https://github.com/w3c/webdriver/issues/1238\u001B[39;00m\n\u001B[0;32m    407\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m cur_host \u001B[38;5;129;01mis\u001B[39;00m cookie\u001B[38;5;241m.\u001B[39mdomain\u001B[38;5;241m.\u001B[39mlstrip(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scholarly\\_proxy_generator.py:356\u001B[0m, in \u001B[0;36mProxyGenerator._get_webdriver\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    353\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mdebug(e)\n\u001B[0;32m    355\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 356\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_firefox_webdriver\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    357\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    358\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot open Firefox/Geckodriver: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, err)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scholarly\\_proxy_generator.py:391\u001B[0m, in \u001B[0;36mProxyGenerator._get_firefox_webdriver\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    389\u001B[0m options \u001B[38;5;241m=\u001B[39m FirefoxOptions()\n\u001B[0;32m    390\u001B[0m options\u001B[38;5;241m.\u001B[39madd_argument(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m--headless\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 391\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_webdriver \u001B[38;5;241m=\u001B[39m \u001B[43mwebdriver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFirefox\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    392\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_webdriver\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://scholar.google.com\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# Need to pre-load to set cookies later\u001B[39;00m\n\u001B[0;32m    394\u001B[0m \u001B[38;5;66;03m# It might make sense to (pre)set cookies as well, e.g., to set a GSP ID.\u001B[39;00m\n\u001B[0;32m    395\u001B[0m \u001B[38;5;66;03m# However, a limitation of webdriver makes it impossible to set cookies for\u001B[39;00m\n\u001B[0;32m    396\u001B[0m \u001B[38;5;66;03m# domains other than the current active one, cf. https://github.com/w3c/webdriver/issues/1238\u001B[39;00m\n\u001B[0;32m    397\u001B[0m \u001B[38;5;66;03m# Therefore setting cookies in the session instance for other domains than the on set above\u001B[39;00m\n\u001B[0;32m    398\u001B[0m \u001B[38;5;66;03m# (e.g., via self._session.cookies.set) will create problems when transferring them to the\u001B[39;00m\n\u001B[0;32m    399\u001B[0m \u001B[38;5;66;03m# webdriver when handling captchas.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\firefox\\webdriver.py:60\u001B[0m, in \u001B[0;36mWebDriver.__init__\u001B[1;34m(self, options, service, keep_alive)\u001B[0m\n\u001B[0;32m     57\u001B[0m options \u001B[38;5;241m=\u001B[39m options \u001B[38;5;28;01mif\u001B[39;00m options \u001B[38;5;28;01melse\u001B[39;00m Options()\n\u001B[0;32m     59\u001B[0m finder \u001B[38;5;241m=\u001B[39m DriverFinder(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mservice, options)\n\u001B[1;32m---> 60\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mfinder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_browser_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m     61\u001B[0m     options\u001B[38;5;241m.\u001B[39mbinary_location \u001B[38;5;241m=\u001B[39m finder\u001B[38;5;241m.\u001B[39mget_browser_path()\n\u001B[0;32m     62\u001B[0m     options\u001B[38;5;241m.\u001B[39mbrowser_version \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:47\u001B[0m, in \u001B[0;36mDriverFinder.get_browser_path\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_browser_path\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_binary_paths\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbrowser_path\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:67\u001B[0m, in \u001B[0;36mDriverFinder._binary_paths\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_paths[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdriver_path\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m path\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 67\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mSeleniumManager\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbinary_paths\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_to_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m Path(output[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdriver_path\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mis_file():\n\u001B[0;32m     69\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_paths[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdriver_path\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m output[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdriver_path\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\selenium_manager.py:53\u001B[0m, in \u001B[0;36mSeleniumManager.binary_paths\u001B[1;34m(self, args)\u001B[0m\n\u001B[0;32m     50\u001B[0m args\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--output\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     51\u001B[0m args\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 53\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\selenium_manager.py:106\u001B[0m, in \u001B[0;36mSeleniumManager._run\u001B[1;34m(args)\u001B[0m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    105\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m sys\u001B[38;5;241m.\u001B[39mplatform \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwin32\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 106\u001B[0m         completed_proc \u001B[38;5;241m=\u001B[39m \u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcapture_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreationflags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCREATE_NO_WINDOW\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    108\u001B[0m         completed_proc \u001B[38;5;241m=\u001B[39m subprocess\u001B[38;5;241m.\u001B[39mrun(args, capture_output\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:550\u001B[0m, in \u001B[0;36mrun\u001B[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001B[0m\n\u001B[0;32m    548\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Popen(\u001B[38;5;241m*\u001B[39mpopenargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;28;01mas\u001B[39;00m process:\n\u001B[0;32m    549\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 550\u001B[0m         stdout, stderr \u001B[38;5;241m=\u001B[39m \u001B[43mprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcommunicate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    551\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m TimeoutExpired \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    552\u001B[0m         process\u001B[38;5;241m.\u001B[39mkill()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1209\u001B[0m, in \u001B[0;36mPopen.communicate\u001B[1;34m(self, input, timeout)\u001B[0m\n\u001B[0;32m   1206\u001B[0m     endtime \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1208\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1209\u001B[0m     stdout, stderr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_communicate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mendtime\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1210\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[0;32m   1211\u001B[0m     \u001B[38;5;66;03m# https://bugs.python.org/issue25942\u001B[39;00m\n\u001B[0;32m   1212\u001B[0m     \u001B[38;5;66;03m# See the detailed comment in .wait().\u001B[39;00m\n\u001B[0;32m   1213\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1626\u001B[0m, in \u001B[0;36mPopen._communicate\u001B[1;34m(self, input, endtime, orig_timeout)\u001B[0m\n\u001B[0;32m   1622\u001B[0m \u001B[38;5;66;03m# Wait for the reader threads, or time out.  If we time out, the\u001B[39;00m\n\u001B[0;32m   1623\u001B[0m \u001B[38;5;66;03m# threads remain reading and the fds left open in case the user\u001B[39;00m\n\u001B[0;32m   1624\u001B[0m \u001B[38;5;66;03m# calls communicate again.\u001B[39;00m\n\u001B[0;32m   1625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1626\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstdout_thread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_remaining_time\u001B[49m\u001B[43m(\u001B[49m\u001B[43mendtime\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1627\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdout_thread\u001B[38;5;241m.\u001B[39mis_alive():\n\u001B[0;32m   1628\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m TimeoutExpired(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, orig_timeout)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:1112\u001B[0m, in \u001B[0;36mThread.join\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m   1109\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot join current thread\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1112\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wait_for_tstate_lock\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1113\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1114\u001B[0m     \u001B[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001B[39;00m\n\u001B[0;32m   1115\u001B[0m     \u001B[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001B[39;00m\n\u001B[0;32m   1116\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait_for_tstate_lock(timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mmax\u001B[39m(timeout, \u001B[38;5;241m0\u001B[39m))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:1132\u001B[0m, in \u001B[0;36mThread._wait_for_tstate_lock\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m   1129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1132\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mlock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblock\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1133\u001B[0m         lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m   1134\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stop()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "url = \"https://scholar.google.com/scholar?cites=6606720413006378435&as_sdt=2005&sciodt=0,5&hl=en\"\n",
    "cites_id_match = re.search(r'cites=(\\d+)', url)\n",
    "results = scholarly.search_citedby(cites_id_match.group(1))\n",
    "\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"Résultat {i+1}:\")\n",
    "    print(\"Titre:\", res[\"bib\"][\"title\"])\n",
    "    print(\"Année de publication:\", res[\"bib\"]['pub_year'])\n",
    "    \n",
    "    if \"num_citations\" in res:\n",
    "        print(\"Nombre de citations:\", res[\"num_citations\"])\n",
    "    else:\n",
    "        print(\"Aucune information sur les citations disponible.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:17:55.321918Z",
     "start_time": "2024-04-26T13:17:18.344248800Z"
    }
   },
   "id": "d160ce8dcea40c0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "454755abcb4a6eff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
