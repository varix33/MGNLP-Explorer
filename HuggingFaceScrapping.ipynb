{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-25T11:14:01.962920900Z",
     "start_time": "2024-03-25T11:13:59.252787700Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import list_datasets\n",
    "from requests_cache import CachedSession\n",
    "import requests\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Extraction des données des datasets d'HuggingFace</h3>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdc02e0adba4c912"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>1) Extraction des métadonnées</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1a899050a79d39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fonction pour récupérer les informations (métadonnées) d'un dataset\n",
    "    name : nom du dataset\n",
    "    session : session avec cache\n",
    "\"\"\"\n",
    "def fetch_dataset_info(name, session):\n",
    "    url = f\"https://huggingface.co/api/datasets/{name}\"\n",
    "\n",
    "    while True:\n",
    "        response = session.get(url, params={\"full\": \"True\"})\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        elif response.status_code == 429:\n",
    "            print(\"Retrying in 15 seconds...\")\n",
    "            time.sleep(15)\n",
    "        else:\n",
    "            return None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "437d16c3864cd4ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Fonction pour récupérer les informations (métadonnées) de plusieurs datasets en parallèle\n",
    "    dataset_names : liste des noms des datasets\n",
    "    session : session avec cache\n",
    "    max_workers : nombre de threads\n",
    "\"\"\"\n",
    "def retrieve_dataset_info(dataset_names,session,max_workers=15):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(fetch_dataset_info, name, session) for name in dataset_names]\n",
    "        for future in tqdm(futures, total=len(dataset_names), desc=\"Progress\"):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4cbb1961663f162"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Récupération des noms de tous les datasets disponibles sur HuggingFace\n",
    "dataset_names = list_datasets()\n",
    "print(\"nb total de dataset disponible sur huggingface :\", len(dataset_names))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e3f4f74ae599257"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Création d'une session avec cache\n",
    "session = CachedSession()\n",
    "# Récupération des informations (métadonnées) des datasets\n",
    "datasets_info = retrieve_dataset_info(dataset_names, session)\n",
    "print(\"nb de dataset ayant des métadonnées :\", len(datasets_info))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f908fd01f032ac06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Création d'un fichier json avec les données des datasets\n",
    "with open(\"datasets_info.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(datasets_info, f, ensure_ascii=False, indent=2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e0c1633fc840e03"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>2) Nettoyage des métadonnées</h4>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2c0e8af65a29969"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122908/122908 [00:09<00:00, 13306.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_info Nettoyé :\n",
      "{'_id': '621ffdd236468d709f181d58', 'id': 'acronym_identification', 'sha': '15ef643450d589d5883e289ffadeb03563e80a9e', 'lastModified': '2024-01-09T11:39:57.000Z', 'private': False, 'gated': False, 'disabled': False, 'paperswithcode_id': 'acronym-identification', 'downloads': 706, 'likes': 18, 'cardData': {'annotations_creators': ['expert-generated'], 'language_creators': ['found'], 'language': ['en'], 'license': ['mit'], 'multilinguality': ['monolingual'], 'size_categories': ['10K<n<100K'], 'source_datasets': ['original'], 'task_categories': ['token-classification'], 'task_ids': [], 'paperswithcode_id': 'acronym-identification', 'pretty_name': 'Acronym Identification Dataset', 'tags': ['acronym-identification'], 'dataset_info': {'features': [{'name': 'id', 'dtype': 'string'}, {'name': 'tokens', 'sequence': 'string'}, {'name': 'labels', 'sequence': {'class_label': {'names': {'0': 'B-long', '1': 'B-short', '2': 'I-long', '3': 'I-short', '4': 'O'}}}}], 'splits': [{'name': 'train', 'num_bytes': 7792771, 'num_examples': 14006}, {'name': 'validation', 'num_bytes': 952689, 'num_examples': 1717}, {'name': 'test', 'num_bytes': 987712, 'num_examples': 1750}], 'download_size': 2071007, 'dataset_size': 9733172}, 'train-eval-index': [{'config': 'default', 'task': 'token-classification', 'task_id': 'entity_extraction', 'splits': {'eval_split': 'test'}, 'col_mapping': {'tokens': 'tokens', 'labels': 'tags'}}]}, 'createdAt': '2022-03-02T23:29:22.000Z'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Lecture du fichier json\n",
    "with open(\"datasets_info.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    datasets_info = json.load(f)\n",
    "\n",
    "# Nettoyage des données\n",
    "datasets_info_clean = []\n",
    "for dataset in tqdm(datasets_info):\n",
    "    if dataset is None:\n",
    "        continue\n",
    "    else:\n",
    "        # On enlève les données inutiles pour réduire la taille du fichier json\n",
    "        if \"cardData\" in dataset and \"configs\" in dataset[\"cardData\"]:\n",
    "            dataset[\"cardData\"].pop(\"configs\")\n",
    "        if \"siblings\" in dataset:\n",
    "            dataset.pop(\"siblings\")\n",
    "        if \"tags\" in dataset:\n",
    "            dataset.pop(\"tags\")\n",
    "        datasets_info_clean.append(dataset)\n",
    "\n",
    "print(\"dataset_info Nettoyé :\")\n",
    "print(datasets_info[0])\n",
    "print(\"Nombre de datasets nettoyés :\", len(datasets_info_clean))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T11:20:44.216833600Z",
     "start_time": "2024-03-25T11:14:01.962920900Z"
    }
   },
   "id": "8bfa6ac0ca4c6b30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>3) Extraction des dataset_cards</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1e62cabe303ce4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fonction pour télécharger le fichier README.md(datasetCard) d'un dataset\n",
    "    repo_id : nom du dataset\n",
    "\"\"\"\n",
    "def download_dataset_card(repo_id):\n",
    "    try:\n",
    "        data_card_path = hf_hub_download(repo_id=repo_id, filename=\"README.md\", repo_type=\"dataset\")\n",
    "        with open(data_card_path, \"r\", encoding=\"utf8\") as f:\n",
    "            dataset_card = f.read()\n",
    "        return dataset_card\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur téléchargement datasetCard {repo_id} :\", str(e))\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a8cbcdd3437e264"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fonction pour récupérer les dataset cards et les ajouter aux métadonnées des datasets (datasets_info)\n",
    "    datasets_info : liste contenant les métadonnées des datasets\n",
    "\"\"\"\n",
    "def retrieve_dataset_cards(datasets_info):\n",
    "    nb_datasets_card = 0\n",
    "\n",
    "    # Liste pour stocker les futurs résultats\n",
    "    futures = []\n",
    "\n",
    "    # Utilisation de ThreadPoolExecutor pour télécharger en parallèle\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        for dataset_info in datasets_info:\n",
    "            if \"cardData\" in dataset_info:\n",
    "                # Soumettre une tâche de téléchargement pour chaque dataset\n",
    "                future = executor.submit(download_dataset_card, dataset_info[\"id\"])\n",
    "                futures.append((dataset_info, future))\n",
    "\n",
    "        # Attendre que toutes les tâches soient terminées\n",
    "        for dataset_info, future in tqdm(futures, total=len(futures), desc=\"Progress\"):\n",
    "            dataset_card = future.result()\n",
    "            if dataset_card:\n",
    "                # Ajouter le dataset_card aux métadonnées du dataset\n",
    "                dataset_info[\"dataset_card\"] = dataset_card\n",
    "                nb_datasets_card += 1\n",
    "\n",
    "    print(f\"Nombre de dataset_card récupérés : {nb_datasets_card} / {len(datasets_info)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53f962ee256083d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Désactivation des warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# Récupération les dataset cards\n",
    "retrieve_dataset_cards(datasets_info_clean)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "482aec5c72a570ef"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Création d'un fichier json avec les métadonnées des datasets + les dataset_card\n",
    "with open(\"datasets_info+card.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(datasets_info, f, ensure_ascii=False, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T11:56:49.146653300Z",
     "start_time": "2024-03-25T11:56:35.449868900Z"
    }
   },
   "id": "4bd244909e2d324f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Traitement des datasetCards</h3>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4644bd2b3f810f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>1) Extraction de la description des datasetCards</h4>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66a75d143d9dcb2c"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de datasets : 122908\n"
     ]
    }
   ],
   "source": [
    "# Lecture du fichier json\n",
    "with open(\"datasets_info+card.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    datasets_info = json.load(f)\n",
    "print(\"Nombre de datasets :\", len(datasets_info))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T12:06:35.021743700Z",
     "start_time": "2024-03-25T12:06:09.311175800Z"
    }
   },
   "id": "a3082ff32cfd59d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_description_from_card(model_card):\n",
    "    description_keywords =\\\n",
    "    [\"Description\", \"description\", \"Summary\", \"summary\", \"Detail\", \"detail\", \"Dataset\", \"dataset\"]\n",
    "    \n",
    "    description = \"\"\n",
    "    lines = model_card.split(\"\\n\")\n",
    "    \n",
    "    for i in range(len(lines)):\n",
    "        if lines[i].startswith(\"#\"):\n",
    "            for description_keyword in description_keywords:\n",
    "                if description_keyword in lines[i]:\n",
    "                    # On récupère la description du dataset\n",
    "                    i+=1\n",
    "                    while i < len(lines) and not lines[i].startswith(\"#\"):\n",
    "                        description = description + lines[i]\n",
    "                        i+=1\n",
    "                    return description\n",
    "    return None "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "686b1be317cc0a9f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Parcourir le fichier json des datasets pour récupérer les dataset cards\n",
    "with open(\"datasets.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    datasets = json.load(f)\n",
    "\n",
    "descriptions = []\n",
    "nb_datasets = len(datasets)\n",
    "nb_descriptions_found = 0\n",
    "# Récupération des descriptions des dataset cards\n",
    "for dataset in tqdm(datasets):\n",
    "    if \"dataset_card\" in dataset:\n",
    "        dataset_card = dataset[\"dataset_card\"]\n",
    "        description = get_description_from_card(dataset_card)\n",
    "        if description:\n",
    "            descriptions.append({\"description\":description, \"id\":dataset[\"id\"]})\n",
    "            nb_descriptions_found += 1\n",
    "\n",
    "print(nb_descriptions_found)\n",
    "print(nb_datasets)\n",
    "print(f\"Pourcentage de dataset cards avec description: {(nb_descriptions_found/nb_datasets)*100:.2f}%\")\n",
    "print(descriptions[:20])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdaf82eedb30f117"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
