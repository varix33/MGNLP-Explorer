{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-26T11:45:51.100892500Z",
     "start_time": "2024-04-26T11:45:45.143696800Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import list_datasets\n",
    "from requests_cache import CachedSession\n",
    "import requests\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Extraction des donn√©es des datasets d'HuggingFace</h3>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdc02e0adba4c912"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>1) Extraction des m√©tadonn√©es</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1a899050a79d39"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fonction pour r√©cup√©rer les informations (m√©tadonn√©es) d'un dataset\n",
    "    name : nom du dataset\n",
    "    session : session avec cache\n",
    "\"\"\"\n",
    "def fetch_dataset_info(name, session):\n",
    "    url = f\"https://huggingface.co/api/datasets/{name}\"\n",
    "\n",
    "    while True:\n",
    "        response = session.get(url, params={\"full\": \"True\"})\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        elif response.status_code == 429:\n",
    "            time.sleep(15)\n",
    "        else:\n",
    "            return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T19:09:35.102456300Z",
     "start_time": "2024-04-25T19:09:35.084086900Z"
    }
   },
   "id": "437d16c3864cd4ce"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Fonction pour r√©cup√©rer les informations (m√©tadonn√©es) de plusieurs datasets en parall√®le\n",
    "    dataset_names : liste des noms des datasets\n",
    "    session : session avec cache\n",
    "    max_workers : nombre de threads\n",
    "\"\"\"\n",
    "def retrieve_dataset_info(dataset_names,session,max_workers=15):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(fetch_dataset_info, name, session) for name in dataset_names]\n",
    "        for future in tqdm(futures, total=len(dataset_names), desc=\"Progress\"):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T12:14:33.174142300Z",
     "start_time": "2024-04-25T12:14:33.145169800Z"
    }
   },
   "id": "f4cbb1961663f162"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antoine\\AppData\\Local\\Temp\\ipykernel_2084\\1704292670.py:2: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  dataset_names = list_datasets()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb total de dataset disponible sur huggingface : 136631\n"
     ]
    }
   ],
   "source": [
    "# R√©cup√©ration des noms de tous les datasets disponibles sur HuggingFace\n",
    "dataset_names = list_datasets()\n",
    "print(\"nb total de dataset disponible sur huggingface :\", len(dataset_names))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T12:15:11.490036600Z",
     "start_time": "2024-04-25T12:14:34.357531400Z"
    }
   },
   "id": "9e3f4f74ae599257"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136631/136631 [1:07:37<00:00, 33.67it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb de dataset ayant des m√©tadonn√©es : 136631\n"
     ]
    }
   ],
   "source": [
    "# Cr√©ation d'une session avec cache\n",
    "session = CachedSession()\n",
    "# R√©cup√©ration des informations (m√©tadonn√©es) des datasets\n",
    "datasets_info = retrieve_dataset_info(dataset_names, session)\n",
    "print(\"nb de dataset ayant des m√©tadonn√©es :\", len(datasets_info))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-25T12:15:11.492035800Z"
    }
   },
   "id": "f908fd01f032ac06"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Cr√©ation d'un fichier json avec les donn√©es des datasets\n",
    "with open(\"datasets_info.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(datasets_info, f, ensure_ascii=False, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T13:26:45.780615600Z",
     "start_time": "2024-04-25T13:23:30.821735Z"
    }
   },
   "id": "6e0c1633fc840e03"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>2) Nettoyage des m√©tadonn√©es</h4>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2c0e8af65a29969"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136631/136631 [00:21<00:00, 6298.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_info Nettoy√© :\n",
      "{'_id': '621ffdd236468d709f181d58', 'id': 'acronym_identification', 'sha': '15ef643450d589d5883e289ffadeb03563e80a9e', 'lastModified': '2024-01-09T11:39:57.000Z', 'private': False, 'gated': False, 'disabled': False, 'description': '\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Acronym Identification Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains the training, validation, and test data for the Shared Task 1: Acronym Identification of the AAAI-21 Workshop on Scientific Document Understanding.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThe dataset supports an acronym-identification task, where the aim is to predic which tokens in a pre-tokenized sentence correspond to acronyms. The dataset was released for a Shared‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/acronym_identification.', 'paperswithcode_id': 'acronym-identification', 'downloads': 632, 'likes': 18, 'cardData': {'annotations_creators': ['expert-generated'], 'language_creators': ['found'], 'language': ['en'], 'license': ['mit'], 'multilinguality': ['monolingual'], 'size_categories': ['10K<n<100K'], 'source_datasets': ['original'], 'task_categories': ['token-classification'], 'task_ids': [], 'paperswithcode_id': 'acronym-identification', 'pretty_name': 'Acronym Identification Dataset', 'tags': ['acronym-identification'], 'dataset_info': {'features': [{'name': 'id', 'dtype': 'string'}, {'name': 'tokens', 'sequence': 'string'}, {'name': 'labels', 'sequence': {'class_label': {'names': {'0': 'B-long', '1': 'B-short', '2': 'I-long', '3': 'I-short', '4': 'O'}}}}], 'splits': [{'name': 'train', 'num_bytes': 7792771, 'num_examples': 14006}, {'name': 'validation', 'num_bytes': 952689, 'num_examples': 1717}, {'name': 'test', 'num_bytes': 987712, 'num_examples': 1750}], 'download_size': 2071007, 'dataset_size': 9733172}, 'train-eval-index': [{'config': 'default', 'task': 'token-classification', 'task_id': 'entity_extraction', 'splits': {'eval_split': 'test'}, 'col_mapping': {'tokens': 'tokens', 'labels': 'tags'}}]}, 'createdAt': '2022-03-02T23:29:22.000Z', 'arxiv': ['2010.14678']}\n",
      "Nombre de datasets nettoy√©s : 133172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#with open(\"datasets_info.json\", \"r\", encoding=\"utf8\") as f:\n",
    "#   datasets_info = json.load(f)\n",
    "\n",
    "# Nettoyage des donn√©es\n",
    "datasets_info_clean = []\n",
    "for dataset in tqdm(datasets_info):\n",
    "    arxiv_list = []\n",
    "    if dataset is None:\n",
    "        continue\n",
    "    else:\n",
    "        # On enl√®ve les donn√©es inutiles pour r√©duire la taille du fichier json\n",
    "        if \"cardData\" in dataset and \"configs\" in dataset[\"cardData\"]:\n",
    "            dataset[\"cardData\"].pop(\"configs\")\n",
    "        if \"siblings\" in dataset:\n",
    "            dataset.pop(\"siblings\")\n",
    "        if \"tags\" in dataset:\n",
    "            for tag in dataset[\"tags\"]:\n",
    "                tag = tag.split(':')\n",
    "                if tag[0] == \"arxiv\" and len(tag) == 2:\n",
    "                    arxiv_list.append(tag[1])\n",
    "        dataset.pop(\"tags\")\n",
    "        if arxiv_list:\n",
    "            dataset[\"arxiv\"] = arxiv_list\n",
    "        datasets_info_clean.append(dataset)\n",
    "\n",
    "print(\"dataset_info Nettoy√© :\")\n",
    "print(datasets_info_clean[0])\n",
    "print(\"Nombre de datasets nettoy√©s :\", len(datasets_info_clean))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T13:29:33.612014200Z",
     "start_time": "2024-04-25T13:29:11.884740100Z"
    }
   },
   "id": "8bfa6ac0ca4c6b30"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "with open(\"datasets_info.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(datasets_info_clean, f, ensure_ascii=False, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T13:30:23.895979Z",
     "start_time": "2024-04-25T13:30:14.494985100Z"
    }
   },
   "id": "cce17190ffba17c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>3) Extraction des dataset_cards</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1e62cabe303ce4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fonction pour t√©l√©charger le fichier README.md(datasetCard) d'un dataset\n",
    "    repo_id : nom du dataset\n",
    "\"\"\"\n",
    "def download_dataset_card(repo_id):\n",
    "    try:\n",
    "        data_card_path = hf_hub_download(repo_id=repo_id, filename=\"README.md\", repo_type=\"dataset\")\n",
    "        with open(data_card_path, \"r\", encoding=\"utf8\") as f:\n",
    "            dataset_card = f.read()\n",
    "        return dataset_card\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur t√©l√©chargement datasetCard {repo_id} :\", str(e))\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T20:04:33.280376200Z",
     "start_time": "2024-04-25T20:04:33.268521700Z"
    }
   },
   "id": "6a8cbcdd3437e264"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fonction pour r√©cup√©rer les dataset cards\n",
    "    datasets_info : liste contenant les m√©tadonn√©es des datasets\n",
    "\"\"\"\n",
    "def retrieve_datasets_card(datasets_info):\n",
    "    nb_datasets_card = 0\n",
    "\n",
    "    # Liste pour stocker les futurs r√©sultats\n",
    "    futures = []\n",
    "    # Liste pour stocker les datasetCards\n",
    "    dataset_card_list = []\n",
    "\n",
    "    # Utilisation de ThreadPoolExecutor pour t√©l√©charger en parall√®le\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        for dataset_info in datasets_info:\n",
    "            if dataset_info and \"cardData\" in dataset_info:\n",
    "                # Soumettre une t√¢che de t√©l√©chargement pour chaque dataset\n",
    "                future = executor.submit(download_dataset_card, dataset_info[\"id\"])\n",
    "                futures.append((dataset_info[\"id\"], future))\n",
    "\n",
    "        # Attendre que toutes les t√¢ches soient termin√©es\n",
    "        for dataset_id, future in tqdm(futures, total=len(futures), desc=\"Progress\"):\n",
    "            dataset_card = future.result()\n",
    "            if dataset_card:\n",
    "                # Ajouter le dataset_card aux m√©tadonn√©es du dataset\n",
    "                dataset_card_list.append({\"id\":dataset_id, \"dataset_card\":dataset_card})\n",
    "                nb_datasets_card += 1\n",
    "    \n",
    "    print(f\"Nombre de dataset_card r√©cup√©r√©s : {nb_datasets_card} / {len(datasets_info)}\")\n",
    "    \n",
    "    return dataset_card_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T20:04:35.147925700Z",
     "start_time": "2024-04-25T20:04:35.127880500Z"
    }
   },
   "id": "53f962ee256083d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   4%|‚ñç         | 3937/90543 [01:06<23:23, 61.72it/s]  "
     ]
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/4.05k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b9268d02a32461b8acbe7517ea98568"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/5.06k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4dd7db3e7404510be2a13fa331c4525"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/4.90k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9f06e198b8c4c58b988334ae329b260"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   6%|‚ñå         | 5036/90543 [01:23<20:48, 68.48it/s]"
     ]
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/404 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66de54907f3b485ab4cb1cbefdb37f69"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/397 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "daee6e0efd5a4fcf9f402aeca24a31f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   6%|‚ñå         | 5230/90543 [01:27<23:55, 59.42it/s]"
     ]
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/202 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "353de75f87bc4102841ceda32eb40dc9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   6%|‚ñã         | 5683/90543 [01:34<21:12, 66.69it/s]"
     ]
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/422 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d4d0a584f4f42eb8de747e3d4e43c94"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   6%|‚ñã         | 5883/90543 [01:37<22:54, 61.60it/s]"
     ]
    }
   ],
   "source": [
    "with open(\"datasets_info.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    datasets_info = json.load(f)\n",
    "\n",
    "# D√©sactivation des warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# R√©cup√©ration des dataset cards\n",
    "datasets_card = retrieve_datasets_card(datasets_info)\n",
    "\n",
    "# Cr√©ation d'un fichier json avec les m√©tadonn√©es des datasets + les dataset_card\n",
    "with open(\"datasets_card.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(datasets_card, f, ensure_ascii=False, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-25T20:04:37.691656600Z"
    }
   },
   "id": "482aec5c72a570ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Traitement des datasetCards</h3>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4644bd2b3f810f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>1) Extraction de la description des datasetCards</h4>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66a75d143d9dcb2c"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_description_from_card(model_card):\n",
    "    description_keywords =\\\n",
    "    [\"Description\", \"description\", \"Summary\", \"summary\", \"Detail\", \"detail\", \"Dataset\", \"dataset\"]\n",
    "    \n",
    "    description = \"\"\n",
    "    lines = model_card.split(\"\\n\")\n",
    "    \n",
    "    for i in range(len(lines)):\n",
    "        if lines[i].startswith(\"#\"):\n",
    "            for description_keyword in description_keywords:\n",
    "                if description_keyword in lines[i]:\n",
    "                    # On r√©cup√®re la description du dataset\n",
    "                    i+=1\n",
    "                    while i < len(lines) and not lines[i].startswith(\"#\"):\n",
    "                        description = description + lines[i]\n",
    "                        i+=1\n",
    "                    return description\n",
    "    return None "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T19:10:29.704782900Z",
     "start_time": "2024-04-25T19:10:29.691596700Z"
    }
   },
   "id": "686b1be317cc0a9f"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90540/90540 [00:01<00:00, 55167.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40298\n",
      "90540\n",
      "Pourcentage de dataset cards avec description: 44.51%\n",
      "[{'description': '<div class=\"course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400\"><p><b>Deprecated:</b> Dataset \"common_voice\" is deprecated and will soon be deleted. Use datasets under <a href=\"https://huggingface.co/mozilla-foundation\">mozilla-foundation</a> organisation instead. For example, you can load <a href=\"https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0\">Common Voice 13</a> dataset via <code>load_dataset(\"mozilla-foundation/common_voice_13_0\", \"en\")</code></p></div>', 'id': 'common_voice'}, {'description': ' ', 'id': 'freebase_qa'}, {'description': '**BAAD16** is an **Authorship Attribution dataset for Bengali Literature**. It was collected and analyzed by the authors of [this paper](https://arxiv.org/abs/2001.05316). It was created by scraping text from an online Bangla e-library using custom web crawler and contains literary works of various famous Bangla writers. It contains novels, stories, series, and other works of 16 authors. Each sample document is created with 750 words. The dataset is imbalanced and resembles real-world scenarios more closely, where not all the authors will have a large number of sample texts. The following table gives more details about the dataset.| Author Name | Number of Samples | Word Count | Unique Word| --- | --- | --- | --- || zahir rayhan        | 185              | 138k | 20k|nazrul               | 223              | 167k | 33k|manik bandhopaddhay  | 469              | 351k | 44k|nihar ronjon gupta   | 476              | 357k | 43k|bongkim              | 562              | 421k | 62k|tarashonkor          | 775              | 581k | 84k|shottojit roy        | 849              | 636k | 67k|shordindu            | 888              | 666k | 84k|toslima nasrin       | 931              | 698k | 76k|shirshendu           | 1048             | 786k | 69k|zafar iqbal          | 1100             | 825k | 53k|robindronath         | 1259             | 944k | 89k|shorotchandra        | 1312             | 984k | 78k|shomresh             | 1408             | 1056k|69k|shunil gongopaddhay  | 1963             | 1472k|109k|humayun ahmed        | 4518             | 3388k |161k**Total**| 17,966|13,474,500 | 590,660**Average**|1,122.875|842,156.25| 71,822.25    ', 'id': 'Aisha/BAAD16'}, {'description': '**BAAD6** is an **Authorship Attribution dataset for Bengali Literature**. It was collected and analyzed by Hemayet et al [[1]](https://ieeexplore.ieee.org/document/8631977). The data was obtained from different online posts and blogs. This dataset is balanced among the 6 Authors with 350 sample texts per author. This is a relatively small dataset but is noisy given the sources it was collected from and its cleaning procedure. Nonetheless, it may help evaluate authorship attribution systems as it resembles texts often available on the Internet. Details about the dataset are given in the table below.| Author | Samples | Word count | Unique word || ------ | ------ | ------ | ------ ||fe|350|357k|53k|| ij | 350 | 391k | 72k| mk | 350 | 377k | 47k| rn | 350 | 231k | 50k| hm | 350 | 555k | 72k| rg | 350 | 391k | 58k**Total** | 2,100 | 2,304,338 | 230,075**Average** | 350 | 384,056.33 | 59,006.67', 'id': 'Aisha/BAAD6'}, {'description': 'üëâ https://observablehq.com/@frasergreenlee/python-lines-dataset#chart', 'id': 'Fraser/python-state-changes'}, {'description': 'The dataset quora_swe is a subset of the automatically translated (MNT) Swedish Semantic Textual Similarity dataset: quora-deduplicates .', 'id': 'Gabriel/quora_swe'}, {'description': '![HUPD-Diagram](https://huggingface.co/datasets/HUPD/hupd/resolve/main/HUPD-Logo.png)', 'id': 'HUPD/hupd'}, {'description': 'Dataset Description:- **Homepage:** https://metatext.io/datasets/wikiner- **Repository:** - **Paper:** https://www.sciencedirect.com/science/article/pii/S0004370212000276?via%3Dihub- **Leaderboard:**- **Point of Contact:**', 'id': 'Jean-Baptiste/wikiner_fr'}, {'description': 'We provide sentential paraphrase detection train, test datasets as well as BERT-based models for the Armenian language.', 'id': 'Karavet/ARPA-Armenian-Paraphrase-Corpus'}, {'description': 'We release a dataset of over 12000 news articles from [iLur.am](http://www.ilur.am/), categorized into 7 classes: sport, politics, weather, economy, accidents, art, society. The articles are split into train (2242k tokens) and test sets (425k tokens).For more details, refer to the [paper](https://arxiv.org/ftp/arxiv/papers/1906/1906.03134.pdf).', 'id': 'Karavet/ILUR-news-text-classification-corpus'}, {'description': 'pioNER corpus provides gold-standard and automatically generated named-entity datasets for the Armenian language.Alongside the datasets, we release 50-, 100-, 200-, and 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.', 'id': 'Karavet/pioNER-Armenian-Named-Entity'}, {'description': 'SQAC is an extractive QA dataset for the Spanish language.- **Paper:** [MarIA: Spanish Language Models](https://upcommons.upc.edu/bitstream/handle/2117/367156/6405-5863-1-PB%20%281%29.pdf?sequence=1)- **Point of Contact:** carlos.rodriguez1@bsc.es- **Leaderboard:** [EvalEs] (https://plantl-gob-es.github.io/spanish-benchmark/)', 'id': 'PlanTL-GOB-ES/SQAC'}, {'description': 'Manually classified collection of Spanish oncological clinical case reports.- **Homepage:** [zenodo](https://zenodo.org/record/3978041)- **Paper:** [Named Entity Recognition, Concept Normalization and Clinical Coding: Overview of the Cantemist Track for Cancer Text Mining in Spanish, Corpus, Guidelines, Methods and Results](https://www.researchgate.net/profile/Antonio-Miranda-Escalada-2/publication/352786464_Named_Entity_Recognition_Concept_Normalization_and_Clinical_Coding_Overview_of_the_Cantemist_Track_for_Cancer_Text_Mining_in_Spanish_Corpus_Guidelines_Methods_and_Results/links/60d98a3b458515d6fbe382d8/Named-Entity-Recognition-Concept-Normalization-and-Clinical-Coding-Overview-of-the-Cantemist-Track-for-Cancer-Text-Mining-in-Spanish-Corpus-Guidelines-Methods-and-Results.pdf)- **Point of Contact:** encargo-pln-life@bsc.es', 'id': 'PlanTL-GOB-ES/cantemist-ner'}, {'description': 'Manually classified collection of Spanish clinical case studies.- **Homepage:** [zenodo](https://zenodo.org/record/4270158)- **Paper:** [PharmaCoNER: Pharmacological Substances, Compounds and proteins Named Entity Recognition track](https://aclanthology.org/D19-5701/)- **Point of Contact:** encargo-pln-life@bsc.es', 'id': 'PlanTL-GOB-ES/pharmaconer'}, {'description': 'persian_news_dataset is a collection of 400k blog posts. these posts have been gathered from more than 10 websites. This dataset can be used in different NLP tasks like language modeling and text generation tasks.This effort is part of a bigger perspective to have several datasets in Persian language for different tasks that have two important factors: `free` and `easy-to-use`. Here is a quick HOW-TO for using this dataset in datasets library:[Demo-datasets](https://saied71.github.io/RohanAiLab/2021/09/03/Demo-datasets.html)', 'id': 'RohanAiLab/persian_blog'}, {'description': 'persian_daily_news  is a collection of 2 million of unique news articles with the headline for each article. dataset can be used in abstractive summarization and paraphrasing tasks.This effort is part of a bigger perspective to have several datasets in Persian language(and other low resources languages) for different tasks that have two important factors: `free` and `easy-to-use`. Here is a quick HOW-TO for using this dataset in datasets library:[Demo-datasets](https://saied71.github.io/RohanAiLab/2021/09/03/Demo-datasets.html)', 'id': 'RohanAiLab/persian_daily_news'}, {'description': 'It includes list of Arabic names with meaning and origin of most names', 'id': 'TRoboto/names'}, {'description': 'Contains aggregated dataset of Russian texts from 6 datasets.', 'id': 'MonoHime/ru_sentiment_dataset'}, {'description': 'Polish CDSCorpus consists of 10K Polish sentence pairs which are human-annotated for semantic relatedness (**CDSC-R**) and entailment (**CDSC-E**). The dataset may be used to evaluate compositional distributional semantics models of Polish. The dataset was presented at ACL 2017. Although the SICK corpus inspires the main design of the dataset, it differs in detail. As in SICK, the sentences come from image captions, but the set of chosen images is much more diverse as they come from 46 thematic groups.', 'id': 'allegro/klej-cdsc-e'}, {'description': 'The Czy wiesz? (eng. Did you know?) the dataset consists of almost 5k question-answer pairs obtained from Czy wiesz... section of Polish Wikipedia. Each question is written by a Wikipedia collaborator and is answered with a link to a relevant Wikipedia article. In huggingface version of this dataset, they chose the negatives which have the largest token overlap with a question.', 'id': 'allegro/klej-dyk'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Parcourir le fichier json des datasets pour r√©cup√©rer les dataset cards\n",
    "with open(\"datasets_card.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    datasets = json.load(f)\n",
    "\n",
    "descriptions = []\n",
    "nb_datasets = len(datasets)\n",
    "nb_descriptions_found = 0\n",
    "\n",
    "# R√©cup√©ration des descriptions des dataset cards\n",
    "for dataset in tqdm(datasets):\n",
    "    #pq datataset peut √™tre None\n",
    "    if dataset is None:\n",
    "        continue\n",
    "    if \"dataset_card\" in dataset:\n",
    "        dataset_card = dataset[\"dataset_card\"]\n",
    "        description = get_description_from_card(dataset_card)\n",
    "        if description:\n",
    "            descriptions.append({\"description\":description, \"id\":dataset[\"id\"]})\n",
    "            nb_descriptions_found += 1\n",
    "\n",
    "print(nb_descriptions_found)\n",
    "print(nb_datasets)\n",
    "print(f\"Pourcentage de dataset cards avec description: {(nb_descriptions_found/nb_datasets)*100:.2f}%\")\n",
    "print(descriptions[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T19:10:42.806383700Z",
     "start_time": "2024-04-25T19:10:34.462460300Z"
    }
   },
   "id": "fdaf82eedb30f117"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>R√©cup√©ration d'information acad√©miques</h3>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c7e5fc1801e80c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>1) R√©cup√©ration des papiers arxiv</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d926b32928ff818"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import xmltodict\n",
    "\"\"\"\n",
    "Fonction pour t√©l√©charger le Papier de recherche correspondant √† un arxiv cit√© dans les m√©ta-donn√©es du dataset\n",
    "    arxiv : num√©ro de l'arxiv\n",
    "\"\"\"\n",
    "def download_arxiv_paper(arxiv):\n",
    "    try:\n",
    "        url = f\"http://export.arxiv.org/api/query?max_results=1&search_query=all:{arxiv}\"\n",
    "        data = urllib.request.urlopen(url)\n",
    "        # Convertir les donn√©es XML en dictionnaire Python\n",
    "        xml_data = data.read().decode('utf-8')\n",
    "        dict_data = xmltodict.parse(xml_data)\n",
    "        return dict_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur t√©l√©chargement du papier arxiv:{arxiv} :\", str(e))\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T22:32:21.828311Z",
     "start_time": "2024-04-25T22:32:21.781084600Z"
    }
   },
   "id": "13215323207e6205"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fonction pour r√©cup√©rer les papiers de recherche arxiv\n",
    "    datasets_info : liste contenant les m√©tadonn√©es des datasets (notamment les num√©ros arxiv)\n",
    "\"\"\"\n",
    "def retrieve_arxiv_paper(datasets_info):\n",
    "    nb_arxiv_paper = 0\n",
    "    nb_arxiv_number = 0\n",
    "\n",
    "    # Liste pour stocker les futurs r√©sultats\n",
    "    futures = []\n",
    "    # Liste pour stocker les datasetCards\n",
    "    arxiv_paper_list = []\n",
    "\n",
    "    # Utilisation de ThreadPoolExecutor pour t√©l√©charger en parall√®le\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        for dataset_info in datasets_info:\n",
    "            if \"arxiv\" in dataset_info:\n",
    "                for arxiv in dataset_info[\"arxiv\"]:\n",
    "                    nb_arxiv_number += 1\n",
    "                    # Soumettre une t√¢che de t√©l√©chargement pour chaque arxiv\n",
    "                    future = executor.submit(download_arxiv_paper, arxiv)\n",
    "                    futures.append((dataset_info[\"id\"], arxiv, future))\n",
    "\n",
    "        # Attendre que toutes les t√¢ches soient termin√©es\n",
    "        for dataset_id, arxiv, future in tqdm(futures, total=len(futures), desc=\"Progress\"):\n",
    "            paper_arxiv = future.result()\n",
    "            if paper_arxiv:\n",
    "                arxiv_paper_list.append({\"id\":dataset_id, \"arxiv\":arxiv, \"paper_arxiv\":paper_arxiv})\n",
    "                nb_arxiv_paper += 1\n",
    "    \n",
    "    print(f\"nb de paper_arxiv r√©cup√©r√©s / nb de num√©ro arxiv r√©cup√©r√©s : {nb_arxiv_paper} / {nb_arxiv_number}\")\n",
    "    \n",
    "    return arxiv_paper_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T22:32:38.326559600Z",
     "start_time": "2024-04-25T22:32:38.293385700Z"
    }
   },
   "id": "5c2cbd7d6e4e69c7"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Lecture du fichier json\n",
    "with open(\"datasets_info.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    datasets_info = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T22:32:46.723791100Z",
     "start_time": "2024-04-25T22:32:40.993311400Z"
    }
   },
   "id": "e640acc55d69af59"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de datasets ayant des num√©ros arxiv : 3581/133172\n"
     ]
    }
   ],
   "source": [
    "# Compte le nombre de datasets ayant des num√©ros arxiv\n",
    "nb_datasets_with_arxiv = 0\n",
    "for dataset in datasets_info:\n",
    "    if \"arxiv\" in dataset:\n",
    "        nb_datasets_with_arxiv += 1\n",
    "print(f\"Nombre de datasets ayant des num√©ros arxiv : {nb_datasets_with_arxiv}/{len(datasets_info)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T22:32:53.166892700Z",
     "start_time": "2024-04-25T22:32:53.074712800Z"
    }
   },
   "id": "69b3a8873dd11982"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retrieve_arxiv_paper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# R√©cup√©ration des papiers de recherche arxiv\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m arxiv_paper_list \u001B[38;5;241m=\u001B[39m \u001B[43mretrieve_arxiv_paper\u001B[49m(datasets_info)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Cr√©ation d'un fichier json contenant les papiers de recherche arxiv\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marxiv_paper.json\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "\u001B[1;31mNameError\u001B[0m: name 'retrieve_arxiv_paper' is not defined"
     ]
    }
   ],
   "source": [
    "# R√©cup√©ration des papiers de recherche arxiv\n",
    "arxiv_paper_list = retrieve_arxiv_paper(datasets_info)\n",
    "\n",
    "# Cr√©ation d'un fichier json contenant les papiers de recherche arxiv\n",
    "with open(\"arxiv_paper.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(arxiv_paper_list, f, ensure_ascii=False, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T22:48:50.214125800Z",
     "start_time": "2024-04-25T22:48:49.976747100Z"
    }
   },
   "id": "b4c4ee944b62a5c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>2) Nettoyage des papiers arxiv</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cf1870de93ca40b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "with open(\"arxiv_paper.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    arxiv_paper_list = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T22:49:03.580317Z",
     "start_time": "2024-04-25T22:49:03.442327200Z"
    }
   },
   "id": "d764b87cf97cdab6"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de papiers arxiv nettoy√©s : 2020\n"
     ]
    }
   ],
   "source": [
    "new_arxiv_paper_list = []\n",
    "for arxiv_paper in arxiv_paper_list:\n",
    "    if arxiv_paper and arxiv_paper[\"paper_arxiv\"] and \"feed\" in arxiv_paper[\"paper_arxiv\"] and \"entry\" in arxiv_paper[\"paper_arxiv\"][\"feed\"]:\n",
    "        new_arxiv_paper_list.append({\"id\": arxiv_paper[\"id\"], \"arxiv\": arxiv_paper[\"arxiv\"], \"paper_arxiv\": arxiv_paper[\"paper_arxiv\"][\"feed\"][\"entry\"]})\n",
    "\n",
    "print(\"Nombre de papiers arxiv nettoy√©s :\", len(new_arxiv_paper_list))\n",
    "with open(\"arxiv_paper.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(new_arxiv_paper_list, f, ensure_ascii=False, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T22:52:27.648872500Z",
     "start_time": "2024-04-25T22:52:27.354670200Z"
    }
   },
   "id": "9296c785b2726cab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>3) R√©cup√©ration des informations sur les citations (Serpapi)</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7975a6a55de61259"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'search_metadata': {'id': '662b9ad55fc49355de6308ca', 'status': 'Success', 'json_endpoint': 'https://serpapi.com/searches/dba6a72319338321/662b9ad55fc49355de6308ca.json', 'created_at': '2024-04-26 12:15:17 UTC', 'processed_at': '2024-04-26 12:15:17 UTC', 'google_scholar_url': 'https://scholar.google.com/scholar?q=arXiv%3A1909.11942&hl=en&num=1', 'raw_html_file': 'https://serpapi.com/searches/dba6a72319338321/662b9ad55fc49355de6308ca.html', 'total_time_taken': 0.98}, 'search_parameters': {'engine': 'google_scholar', 'q': 'arXiv:1909.11942', 'hl': 'en', 'num': '1'}, 'search_information': {'organic_results_state': 'Results for exact spelling', 'query_displayed': 'arXiv:1909.11942'}, 'profiles': {'link': 'https://scholar.google.com/scholar?lookup=0&q=arXiv:1909.11942&hl=en&num=1&as_sdt=0,11', 'serpapi_link': 'https://serpapi.com/search.json?engine=google_scholar_profiles&hl=en&mauthors=arXiv%3A1909.11942'}, 'organic_results': [{'position': 0, 'title': 'Albert: A lite bert for self-supervised learning of language representations', 'result_id': 'wzWRMzbJr1sJ', 'link': 'https://arxiv.org/abs/1909.11942', 'snippet': 'Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the ‚Ä¶', 'publication_info': {'summary': 'Z Lan, M Chen, S Goodman, K Gimpel‚Ä¶ - arXiv preprint arXiv ‚Ä¶, 2019 - arxiv.org', 'authors': [{'name': 'Z Lan', 'link': 'https://scholar.google.com/citations?user=tlDABkgAAAAJ&hl=en&num=1&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=tlDABkgAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'tlDABkgAAAAJ'}, {'name': 'M Chen', 'link': 'https://scholar.google.com/citations?user=aRncxakAAAAJ&hl=en&num=1&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=aRncxakAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'aRncxakAAAAJ'}, {'name': 'S Goodman', 'link': 'https://scholar.google.com/citations?user=xgZ6V-sAAAAJ&hl=en&num=1&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=xgZ6V-sAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'xgZ6V-sAAAAJ'}, {'name': 'K Gimpel', 'link': 'https://scholar.google.com/citations?user=kDHs7DYAAAAJ&hl=en&num=1&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=kDHs7DYAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'kDHs7DYAAAAJ'}]}, 'resources': [{'title': 'arxiv.org', 'file_format': 'PDF', 'link': 'https://arxiv.org/pdf/1909.11942.pdf%3E,'}], 'inline_links': {'serpapi_cite_link': 'https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=wzWRMzbJr1sJ', 'cited_by': {'total': 6757, 'link': 'https://scholar.google.com/scholar?cites=6606720413006378435&as_sdt=80005&sciodt=0,11&hl=en&num=1', 'cites_id': '6606720413006378435', 'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=80005&cites=6606720413006378435&engine=google_scholar&hl=en&num=1'}, 'related_pages_link': 'https://scholar.google.com/scholar?q=related:wzWRMzbJr1sJ:scholar.google.com/&scioq=arXiv:1909.11942&hl=en&num=1&as_sdt=0,11', 'serpapi_related_pages_link': 'https://serpapi.com/search.json?as_sdt=0%2C11&engine=google_scholar&hl=en&num=1&q=related%3AwzWRMzbJr1sJ%3Ascholar.google.com%2F', 'versions': {'total': 10, 'link': 'https://scholar.google.com/scholar?cluster=6606720413006378435&hl=en&num=1&as_sdt=0,11', 'cluster_id': '6606720413006378435', 'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=0%2C11&cluster=6606720413006378435&engine=google_scholar&hl=en&num=1'}, 'cached_page_link': 'https://scholar.googleusercontent.com/scholar?q=cache:wzWRMzbJr1sJ:scholar.google.com/+arXiv:1909.11942&hl=en&num=1&as_sdt=0,11'}}]}\n"
     ]
    }
   ],
   "source": [
    "#refered by:\n",
    "# import serpapi\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "params = {\n",
    "    \"engine\": \"google_scholar\",\n",
    "    \"num\": \"1\",\n",
    "    \"q\": \"arXiv:1909.11942\",\n",
    "    \"hl\": \"en\",\n",
    "    \"api_key\": \"5c031d347fae722e2e6576c726ff739ccf6d55165001bb936c95cfa7e15a5994\"\n",
    "}\n",
    "\n",
    "search = GoogleSearch(params)\n",
    "results = search.get_dict()\n",
    "if results:\n",
    "    if \"inline_links\" in results and \"cited_by\" in results[\"inline_links\"]:\n",
    "        print(results[\"inline_links\"][\"cited_by\"])\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T12:18:54.124946500Z",
     "start_time": "2024-04-26T12:18:53.320501300Z"
    }
   },
   "id": "93fabba03d9c01b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>4) R√©cup√©ration des informations sur les citations (Scholarly)</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8739e0a3f294c311"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Albert: A lite bert for self-supervised learning of language representations', 'author': ['Z Lan', 'M Chen', 'S Goodman', 'K Gimpel'], 'pub_year': '2019', 'venue': 'arXiv preprint arXiv ‚Ä¶', 'abstract': 'Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/1909.11942', 'author_id': ['tlDABkgAAAAJ', 'aRncxakAAAAJ', 'xgZ6V-sAAAAJ', 'kDHs7DYAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:wzWRMzbJr1sJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DarXiv:1909.11942%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=wzWRMzbJr1sJ&ei=pqgrZvjdE5SCy9YP29Cc0AY&json=', 'num_citations': 6757, 'citedby_url': '/scholar?cites=6606720413006378435&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:wzWRMzbJr1sJ:scholar.google.com/&scioq=arXiv:1909.11942&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/1909.11942.pdf%3E,'}\n"
     ]
    }
   ],
   "source": [
    "from scholarly import scholarly\n",
    "\n",
    "# D√©finir le terme de recherche\n",
    "search_query = 'arXiv:1909.11942'\n",
    "# Effectuer la recherche\n",
    "search_results = scholarly.search_pubs(search_query)\n",
    "\n",
    "# Afficher les informations sur les citations pour chaque r√©sultat de la recherche\n",
    "for i, result in enumerate(search_results):\n",
    "    print(result)\n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"R√©sultat {i+1}:\")\n",
    "    print(\"Ann√©e de publication:\", result[\"bib\"]['pub_year'])\n",
    "    print(\"url citation:\", \"https://scholar.google.com\"+result[\"citedby_url\"])\n",
    "    \n",
    "    if \"num_citations\" in result:\n",
    "        print(\"Nombre de citations:\", result[\"num_citations\"])\n",
    "    else:\n",
    "        print(\"Aucune information sur les citations disponible.\")\n",
    "    \n",
    "    results = scholarly.search_citedby(\"6606720413006378435\")\n",
    "    for i, res in enumerate(results):\n",
    "        print(f\"R√©sultat {i+1}:\")\n",
    "        print(\"Titre:\", res[\"bib\"][\"title\"])\n",
    "        print(\"Ann√©e de publication:\", res[\"bib\"]['pub_year'])\n",
    "        \n",
    "        if \"num_citations\" in res:\n",
    "            print(\"Nombre de citations:\", res[\"num_citations\"])\n",
    "        else:\n",
    "            print(\"Aucune information sur les citations disponible.\")\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:14:15.609890600Z",
     "start_time": "2024-04-26T13:14:13.355423800Z"
    }
   },
   "id": "ac9801ece73ee1ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>5) R√©cup√©ration des papiers de recherche citant le dataset (Serpapi)</h4>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8321d2c329397875"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6606720413006378435\n",
      "R√©sultat 1:\n",
      "Titre: Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\n",
      "Ann√©e de publication: 2023\n",
      "Nombre de citations: 3114\n",
      "R√©sultat 2:\n",
      "Titre: Deep learning--based text classification: a comprehensive review\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 1475\n",
      "R√©sultat 3:\n",
      "Titre: Lamda: Language models for dialog applications\n",
      "Ann√©e de publication: 2022\n",
      "Nombre de citations: 1096\n",
      "R√©sultat 4:\n",
      "Titre: Flashattention: Fast and memory-efficient exact attention with io-awareness\n",
      "Ann√©e de publication: 2022\n",
      "Nombre de citations: 717\n",
      "R√©sultat 5:\n",
      "Titre: Vivit: A video vision transformer\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 1817\n",
      "R√©sultat 6:\n",
      "Titre: On the dangers of stochastic parrots: Can language models be too big?ü¶ú\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 3557\n",
      "R√©sultat 7:\n",
      "Titre: Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\n",
      "Ann√©e de publication: 2022\n",
      "Nombre de citations: 734\n",
      "R√©sultat 8:\n",
      "Titre: BERTopic: Neural topic modeling with a class-based TF-IDF procedure\n",
      "Ann√©e de publication: 2022\n",
      "Nombre de citations: 1094\n",
      "R√©sultat 9:\n",
      "Titre: A survey on vision transformer\n",
      "Ann√©e de publication: 2022\n",
      "Nombre de citations: 1402\n",
      "R√©sultat 10:\n",
      "Titre: Language models are few-shot learners\n",
      "Ann√©e de publication: 2020\n",
      "Nombre de citations: 24464\n",
      "R√©sultat 11:\n",
      "Titre: Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension\n",
      "Ann√©e de publication: 2019\n",
      "Nombre de citations: 9039\n",
      "R√©sultat 12:\n",
      "Titre: Deberta: Decoding-enhanced bert with disentangled attention\n",
      "Ann√©e de publication: 2020\n",
      "Nombre de citations: 1941\n",
      "R√©sultat 13:\n",
      "Titre: Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 567\n",
      "R√©sultat 14:\n",
      "Titre: Xlnet: Generalized autoregressive pretraining for language understanding\n",
      "Ann√©e de publication: 2019\n",
      "Nombre de citations: 9074\n",
      "R√©sultat 15:\n",
      "Titre: Exploring the limits of transfer learning with a unified text-to-text transformer\n",
      "Ann√©e de publication: 2020\n",
      "Nombre de citations: 14778\n",
      "R√©sultat 16:\n",
      "Titre: Self-supervised learning: Generative or contrastive\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 1459\n",
      "R√©sultat 17:\n",
      "Titre: Efficient transformers: A survey\n",
      "Ann√©e de publication: 2022\n",
      "Nombre de citations: 1054\n",
      "R√©sultat 18:\n",
      "Titre: Pre-trained models for natural language processing: A survey\n",
      "Ann√©e de publication: 2020\n",
      "Nombre de citations: 1556\n",
      "R√©sultat 19:\n",
      "Titre: Roformer: Enhanced transformer with rotary position embedding\n",
      "Ann√©e de publication: 2024\n",
      "Nombre de citations: 727\n",
      "R√©sultat 20:\n",
      "Titre: Electra: Pre-training text encoders as discriminators rather than generators\n",
      "Ann√©e de publication: 2020\n",
      "Nombre de citations: 3645\n",
      "R√©sultat 21:\n",
      "Titre: Maxvit: Multi-axis vision transformer\n",
      "Ann√©e de publication: 2022\n",
      "Nombre de citations: 390\n",
      "R√©sultat 22:\n",
      "Titre: Perceiver: General perception with iterative attention\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 741\n",
      "R√©sultat 23:\n",
      "Titre: Recent advances in natural language processing via large pre-trained language models: A survey\n",
      "Ann√©e de publication: 2023\n",
      "Nombre de citations: 438\n",
      "R√©sultat 24:\n",
      "Titre: Transformers are rnns: Fast autoregressive transformers with linear attention\n",
      "Ann√©e de publication: 2020\n",
      "Nombre de citations: 1139\n",
      "R√©sultat 25:\n",
      "Titre: Glm: General language model pretraining with autoregressive blank infilling\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 686\n",
      "R√©sultat 26:\n",
      "Titre: Prottrans: Toward understanding the language of life through self-supervised learning\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 1083\n",
      "R√©sultat 27:\n",
      "Titre: Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 691\n",
      "R√©sultat 28:\n",
      "Titre: A large language model for electronic health records\n",
      "Ann√©e de publication: 2022\n",
      "Nombre de citations: 226\n",
      "R√©sultat 29:\n",
      "Titre: A comprehensive survey on pretrained foundation models: A history from bert to chatgpt\n",
      "Ann√©e de publication: 2023\n",
      "Nombre de citations: 313\n",
      "R√©sultat 30:\n",
      "Titre: Self-supervised graph learning for recommendation\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 766\n",
      "R√©sultat 31:\n",
      "Titre: Less is more: Clipbert for video-and-language learning via sparse sampling\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 582\n",
      "R√©sultat 32:\n",
      "Titre: It's not just size that matters: Small language models are also few-shot learners\n",
      "Ann√©e de publication: 2020\n",
      "Nombre de citations: 769\n",
      "R√©sultat 33:\n",
      "Titre: Uniter: Universal image-text representation learning\n",
      "Ann√©e de publication: 2020\n",
      "Nombre de citations: 1884\n",
      "R√©sultat 34:\n",
      "Titre: Pre-trained models: Past, present and future\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 586\n",
      "R√©sultat 35:\n",
      "Titre: Underspecification presents challenges for credibility in modern machine learning\n",
      "Ann√©e de publication: 2022\n",
      "Nombre de citations: 666\n",
      "R√©sultat 36:\n",
      "Titre: A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt\n",
      "Ann√©e de publication: 2023\n",
      "Nombre de citations: 362\n",
      "R√©sultat 37:\n",
      "Titre: A primer in BERTology: What we know about how BERT works\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 1452\n",
      "R√©sultat 38:\n",
      "Titre: Llm-pruner: On the structural pruning of large language models\n",
      "Ann√©e de publication: 2023\n",
      "Nombre de citations: 120\n",
      "R√©sultat 39:\n",
      "Titre: Tinybert: Distilling bert for natural language understanding\n",
      "Ann√©e de publication: 2019\n",
      "Nombre de citations: 1603\n",
      "R√©sultat 40:\n",
      "Titre: An empirical study of training end-to-end vision-and-language transformers\n",
      "Ann√©e de publication: 2022\n",
      "Nombre de citations: 293\n",
      "R√©sultat 41:\n",
      "Titre: Vision-language pre-training: Basics, recent advances, and future trends\n",
      "Ann√©e de publication: 2022\n",
      "Nombre de citations: 125\n",
      "R√©sultat 42:\n",
      "Titre: A brief overview of ChatGPT: The history, status quo and potential future development\n",
      "Ann√©e de publication: 2023\n",
      "Nombre de citations: 263\n",
      "R√©sultat 43:\n",
      "Titre: LUKE: Deep contextualized entity representations with entity-aware self-attention\n",
      "Ann√©e de publication: 2020\n",
      "Nombre de citations: 664\n",
      "R√©sultat 44:\n",
      "Titre: Spanish pre-trained bert model and evaluation data\n",
      "Ann√©e de publication: 2023\n",
      "Nombre de citations: 791\n",
      "R√©sultat 45:\n",
      "Titre: LEGAL-BERT: The muppets straight out of law school\n",
      "Ann√©e de publication: 2020\n",
      "Nombre de citations: 673\n",
      "R√©sultat 46:\n",
      "Titre: Zeroquant: Efficient and affordable post-training quantization for large-scale transformers\n",
      "Ann√©e de publication: 2022\n",
      "Nombre de citations: 180\n",
      "R√©sultat 47:\n",
      "Titre: ABCDM: An attention-based bidirectional CNN-RNN deep model for sentiment analysis\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 610\n",
      "R√©sultat 48:\n",
      "Titre: Unifying vision-and-language tasks via text generation\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 440\n",
      "R√©sultat 49:\n",
      "Titre: Megatron-lm: Training multi-billion parameter language models using model parallelism\n",
      "Ann√©e de publication: 2019\n",
      "Nombre de citations: 1288\n",
      "R√©sultat 50:\n",
      "Titre: Revisiting pre-trained models for Chinese natural language processing\n",
      "Ann√©e de publication: 2020\n",
      "Nombre de citations: 650\n",
      "R√©sultat 51:\n",
      "Titre: QA-GNN: Reasoning with language models and knowledge graphs for question answering\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 440\n",
      "R√©sultat 52:\n",
      "Titre: Pre-training with whole word masking for chinese bert\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 1210\n",
      "R√©sultat 53:\n",
      "Titre: ProteinBERT: a universal deep-learning model of protein sequence and function\n",
      "Ann√©e de publication: 2022\n",
      "Nombre de citations: 336\n",
      "R√©sultat 54:\n",
      "Titre: An introduction to deep learning in natural language processing: Models, techniques, and tools\n",
      "Ann√©e de publication: 2022\n",
      "Nombre de citations: 324\n",
      "R√©sultat 55:\n",
      "Titre: Agentformer: Agent-aware transformers for socio-temporal multi-agent forecasting\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 355\n",
      "R√©sultat 56:\n",
      "Titre: Siren's song in the AI ocean: a survey on hallucination in large language models\n",
      "Ann√©e de publication: 2023\n",
      "Nombre de citations: 277\n",
      "R√©sultat 57:\n",
      "Titre: Graph neural networks: foundation, frontiers and applications\n",
      "Ann√©e de publication: 2022\n",
      "Nombre de citations: 273\n",
      "R√©sultat 58:\n",
      "Titre: Vitae: Vision transformer advanced by exploring intrinsic inductive bias\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 290\n",
      "R√©sultat 59:\n",
      "Titre: True few-shot learning with language models\n",
      "Ann√©e de publication: 2021\n",
      "Nombre de citations: 312\n",
      "R√©sultat 60:\n",
      "Titre: Merlot reserve: Neural script knowledge through vision and language and sound\n",
      "Ann√©e de publication: 2022\n",
      "Nombre de citations: 195\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(cites_id_match\u001B[38;5;241m.\u001B[39mgroup(\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m      7\u001B[0m results \u001B[38;5;241m=\u001B[39m scholarly\u001B[38;5;241m.\u001B[39msearch_citedby(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m6606720413006378435\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mres\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mresults\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mR√©sultat \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m:\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTitre:\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mres\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbib\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtitle\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scholarly\\publication_parser.py:90\u001B[0m, in \u001B[0;36m_SearchScholarIterator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     87\u001B[0m     url \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_soup\u001B[38;5;241m.\u001B[39mfind(\n\u001B[0;32m     88\u001B[0m         class_\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgs_ico gs_ico_nav_next\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mparent[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhref\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     89\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_url \u001B[38;5;241m=\u001B[39m url\n\u001B[1;32m---> 90\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_url\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     91\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__next__\u001B[39m()\n\u001B[0;32m     92\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scholarly\\publication_parser.py:59\u001B[0m, in \u001B[0;36m_SearchScholarIterator._load_url\u001B[1;34m(self, url)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load_url\u001B[39m(\u001B[38;5;28mself\u001B[39m, url: \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;66;03m# this is temporary until setup json file\u001B[39;00m\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_soup \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nav\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_soup\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pos \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rows \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_soup\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiv\u001B[39m\u001B[38;5;124m'\u001B[39m, class_\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgs_r gs_or gs_scl\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_soup\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiv\u001B[39m\u001B[38;5;124m'\u001B[39m, class_\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgsc_mpat_ttl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scholarly\\_navigator.py:239\u001B[0m, in \u001B[0;36mNavigator._get_soup\u001B[1;34m(self, url)\u001B[0m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_soup\u001B[39m(\u001B[38;5;28mself\u001B[39m, url: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BeautifulSoup:\n\u001B[0;32m    238\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the BeautifulSoup for a page on scholar.google.com\"\"\"\u001B[39;00m\n\u001B[1;32m--> 239\u001B[0m     html \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_page\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhttps://scholar.google.com\u001B[39;49m\u001B[38;5;132;43;01m{0}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    240\u001B[0m     html \u001B[38;5;241m=\u001B[39m html\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\xa0\u001B[39;00m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    241\u001B[0m     res \u001B[38;5;241m=\u001B[39m BeautifulSoup(html, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhtml.parser\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scholarly\\_navigator.py:132\u001B[0m, in \u001B[0;36mNavigator._get_page\u001B[1;34m(self, pagerequest, premium)\u001B[0m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m has_captcha:\n\u001B[0;32m    131\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGot a captcha request.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 132\u001B[0m     session \u001B[38;5;241m=\u001B[39m \u001B[43mpm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_captcha2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpagerequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# Retry request within same session\u001B[39;00m\n\u001B[0;32m    134\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m403\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scholarly\\_proxy_generator.py:404\u001B[0m, in \u001B[0;36mProxyGenerator._handle_captcha2\u001B[1;34m(self, url)\u001B[0m\n\u001B[0;32m    403\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_handle_captcha2\u001B[39m(\u001B[38;5;28mself\u001B[39m, url):\n\u001B[1;32m--> 404\u001B[0m     cur_host \u001B[38;5;241m=\u001B[39m urlparse(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_webdriver\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcurrent_url)\u001B[38;5;241m.\u001B[39mhostname\n\u001B[0;32m    405\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m cookie \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mcookies:\n\u001B[0;32m    406\u001B[0m         \u001B[38;5;66;03m# Only set cookies matching the current domain, cf. https://github.com/w3c/webdriver/issues/1238\u001B[39;00m\n\u001B[0;32m    407\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m cur_host \u001B[38;5;129;01mis\u001B[39;00m cookie\u001B[38;5;241m.\u001B[39mdomain\u001B[38;5;241m.\u001B[39mlstrip(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scholarly\\_proxy_generator.py:356\u001B[0m, in \u001B[0;36mProxyGenerator._get_webdriver\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    353\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mdebug(e)\n\u001B[0;32m    355\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 356\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_firefox_webdriver\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    357\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    358\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot open Firefox/Geckodriver: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, err)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scholarly\\_proxy_generator.py:391\u001B[0m, in \u001B[0;36mProxyGenerator._get_firefox_webdriver\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    389\u001B[0m options \u001B[38;5;241m=\u001B[39m FirefoxOptions()\n\u001B[0;32m    390\u001B[0m options\u001B[38;5;241m.\u001B[39madd_argument(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m--headless\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 391\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_webdriver \u001B[38;5;241m=\u001B[39m \u001B[43mwebdriver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFirefox\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    392\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_webdriver\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://scholar.google.com\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# Need to pre-load to set cookies later\u001B[39;00m\n\u001B[0;32m    394\u001B[0m \u001B[38;5;66;03m# It might make sense to (pre)set cookies as well, e.g., to set a GSP ID.\u001B[39;00m\n\u001B[0;32m    395\u001B[0m \u001B[38;5;66;03m# However, a limitation of webdriver makes it impossible to set cookies for\u001B[39;00m\n\u001B[0;32m    396\u001B[0m \u001B[38;5;66;03m# domains other than the current active one, cf. https://github.com/w3c/webdriver/issues/1238\u001B[39;00m\n\u001B[0;32m    397\u001B[0m \u001B[38;5;66;03m# Therefore setting cookies in the session instance for other domains than the on set above\u001B[39;00m\n\u001B[0;32m    398\u001B[0m \u001B[38;5;66;03m# (e.g., via self._session.cookies.set) will create problems when transferring them to the\u001B[39;00m\n\u001B[0;32m    399\u001B[0m \u001B[38;5;66;03m# webdriver when handling captchas.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\firefox\\webdriver.py:60\u001B[0m, in \u001B[0;36mWebDriver.__init__\u001B[1;34m(self, options, service, keep_alive)\u001B[0m\n\u001B[0;32m     57\u001B[0m options \u001B[38;5;241m=\u001B[39m options \u001B[38;5;28;01mif\u001B[39;00m options \u001B[38;5;28;01melse\u001B[39;00m Options()\n\u001B[0;32m     59\u001B[0m finder \u001B[38;5;241m=\u001B[39m DriverFinder(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mservice, options)\n\u001B[1;32m---> 60\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mfinder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_browser_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m     61\u001B[0m     options\u001B[38;5;241m.\u001B[39mbinary_location \u001B[38;5;241m=\u001B[39m finder\u001B[38;5;241m.\u001B[39mget_browser_path()\n\u001B[0;32m     62\u001B[0m     options\u001B[38;5;241m.\u001B[39mbrowser_version \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:47\u001B[0m, in \u001B[0;36mDriverFinder.get_browser_path\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_browser_path\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_binary_paths\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbrowser_path\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:67\u001B[0m, in \u001B[0;36mDriverFinder._binary_paths\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_paths[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdriver_path\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m path\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 67\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mSeleniumManager\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbinary_paths\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_to_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m Path(output[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdriver_path\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mis_file():\n\u001B[0;32m     69\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_paths[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdriver_path\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m output[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdriver_path\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\selenium_manager.py:53\u001B[0m, in \u001B[0;36mSeleniumManager.binary_paths\u001B[1;34m(self, args)\u001B[0m\n\u001B[0;32m     50\u001B[0m args\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--output\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     51\u001B[0m args\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 53\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\selenium_manager.py:106\u001B[0m, in \u001B[0;36mSeleniumManager._run\u001B[1;34m(args)\u001B[0m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    105\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m sys\u001B[38;5;241m.\u001B[39mplatform \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwin32\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 106\u001B[0m         completed_proc \u001B[38;5;241m=\u001B[39m \u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcapture_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreationflags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCREATE_NO_WINDOW\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    108\u001B[0m         completed_proc \u001B[38;5;241m=\u001B[39m subprocess\u001B[38;5;241m.\u001B[39mrun(args, capture_output\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:550\u001B[0m, in \u001B[0;36mrun\u001B[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001B[0m\n\u001B[0;32m    548\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Popen(\u001B[38;5;241m*\u001B[39mpopenargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;28;01mas\u001B[39;00m process:\n\u001B[0;32m    549\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 550\u001B[0m         stdout, stderr \u001B[38;5;241m=\u001B[39m \u001B[43mprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcommunicate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    551\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m TimeoutExpired \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    552\u001B[0m         process\u001B[38;5;241m.\u001B[39mkill()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1209\u001B[0m, in \u001B[0;36mPopen.communicate\u001B[1;34m(self, input, timeout)\u001B[0m\n\u001B[0;32m   1206\u001B[0m     endtime \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1208\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1209\u001B[0m     stdout, stderr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_communicate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mendtime\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1210\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[0;32m   1211\u001B[0m     \u001B[38;5;66;03m# https://bugs.python.org/issue25942\u001B[39;00m\n\u001B[0;32m   1212\u001B[0m     \u001B[38;5;66;03m# See the detailed comment in .wait().\u001B[39;00m\n\u001B[0;32m   1213\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1626\u001B[0m, in \u001B[0;36mPopen._communicate\u001B[1;34m(self, input, endtime, orig_timeout)\u001B[0m\n\u001B[0;32m   1622\u001B[0m \u001B[38;5;66;03m# Wait for the reader threads, or time out.  If we time out, the\u001B[39;00m\n\u001B[0;32m   1623\u001B[0m \u001B[38;5;66;03m# threads remain reading and the fds left open in case the user\u001B[39;00m\n\u001B[0;32m   1624\u001B[0m \u001B[38;5;66;03m# calls communicate again.\u001B[39;00m\n\u001B[0;32m   1625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1626\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstdout_thread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_remaining_time\u001B[49m\u001B[43m(\u001B[49m\u001B[43mendtime\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1627\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdout_thread\u001B[38;5;241m.\u001B[39mis_alive():\n\u001B[0;32m   1628\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m TimeoutExpired(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, orig_timeout)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:1112\u001B[0m, in \u001B[0;36mThread.join\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m   1109\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot join current thread\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1112\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wait_for_tstate_lock\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1113\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1114\u001B[0m     \u001B[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001B[39;00m\n\u001B[0;32m   1115\u001B[0m     \u001B[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001B[39;00m\n\u001B[0;32m   1116\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait_for_tstate_lock(timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mmax\u001B[39m(timeout, \u001B[38;5;241m0\u001B[39m))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:1132\u001B[0m, in \u001B[0;36mThread._wait_for_tstate_lock\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m   1129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1132\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mlock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblock\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1133\u001B[0m         lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m   1134\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stop()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "url = \"https://scholar.google.com/scholar?cites=6606720413006378435&as_sdt=2005&sciodt=0,5&hl=en\"\n",
    "cites_id_match = re.search(r'cites=(\\d+)', url)\n",
    "results = scholarly.search_citedby(cites_id_match.group(1))\n",
    "\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"R√©sultat {i+1}:\")\n",
    "    print(\"Titre:\", res[\"bib\"][\"title\"])\n",
    "    print(\"Ann√©e de publication:\", res[\"bib\"]['pub_year'])\n",
    "    \n",
    "    if \"num_citations\" in res:\n",
    "        print(\"Nombre de citations:\", res[\"num_citations\"])\n",
    "    else:\n",
    "        print(\"Aucune information sur les citations disponible.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:17:55.321918Z",
     "start_time": "2024-04-26T13:17:18.344248800Z"
    }
   },
   "id": "d160ce8dcea40c0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "454755abcb4a6eff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
